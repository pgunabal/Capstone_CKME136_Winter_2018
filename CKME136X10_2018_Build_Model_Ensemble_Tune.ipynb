{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 25\n",
    "pd.options.display.max_columns  = 25\n",
    "import time\n",
    "import pickle # allows for model to be saved/load to file\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function converts the data frame to the appropriate data type\n",
    "def convert_type(data):\n",
    "    data = data.astype('category')\n",
    "    data['C_MNTH'] = data['C_MNTH'].astype(CategoricalDtype(ordered=True))\n",
    "    data['C_WDAY'] = data['C_WDAY'].astype(CategoricalDtype(ordered=True))\n",
    "    data['C_HOUR'] = data['C_HOUR'].astype(CategoricalDtype(ordered=True))\n",
    "    data['C_VEHS'] = data['C_VEHS'].astype(CategoricalDtype(ordered=True))\n",
    "    data['P_AGE'] = data['P_AGE'].astype(CategoricalDtype(ordered=True))\n",
    "    data['P_PSN'] = data['P_PSN'].astype(CategoricalDtype(ordered=True))\n",
    "    data['P_ISEV'] = data['P_ISEV'].astype('int')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Algorithm Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Algorithms\n",
    "enable_model_xgboost = True\n",
    "enable_model_randomForest = True\n",
    "enable_multiclass_model = True\n",
    "enable_mlp = True\n",
    "\n",
    "predict_xgboost = True\n",
    "predict_randomForest = True\n",
    "predict_mlp = True\n",
    "\n",
    "#Debug\n",
    "verbose_level=1\n",
    "#Multiclass classification, binary if falase\n",
    "multiclass = False\n",
    "over_sample = True\n",
    "\n",
    "if multiclass:\n",
    "    labels=[2, 1, 0]\n",
    "else:\n",
    "    labels=[1, 0]\n",
    "\n",
    "# Datafile\n",
    "#inputfile = 'CKME136X10_2018_Data_CTF.csv'\n",
    "if multiclass:\n",
    "    inputfile_train_O = 'CKME136X10_2018_Data_CTFB_M_O_Train.csv'\n",
    "    inputfile_train_U = 'CKME136X10_2018_Data_CTFB_M_U_Train.csv'\n",
    "    inputfile_test = 'CKME136X10_2018_Data_CTFB_M_Test.csv'\n",
    "else:\n",
    "    inputfile_train_O = 'CKME136X10_2018_Data_CTFB_B_O_Train.csv'\n",
    "    inputfile_train_U = 'CKME136X10_2018_Data_CTFB_B_U_Train.csv'\n",
    "    inputfile_test = 'CKME136X10_2018_Data_CTFB_B_Test.csv'\n",
    "\n",
    "if over_sample:\n",
    "    datafile_train = inputfile_train_O\n",
    "else:\n",
    "    datafile_train = inputfile_train_U\n",
    "\n",
    "datafile_test = inputfile_test\n",
    "\n",
    "\n",
    "#file_input = 'NCDB_FULL_Removed_All_Missing_Values_Binary_Class_Transformed.csv'\n",
    "\n",
    "model_max_iter = 1000\n",
    "datestr = 'dec_10_binary_run_1000_BO_ensamble'\n",
    "\n",
    "# Model File Names for storage\n",
    "file_random_forest = 'random_forest_'  + datestr + '.model'\n",
    "file_xgboost = 'xgboost_'  + datestr + '.model'\n",
    "file_mlp = 'mlp_'  + datestr + '.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   C_MNTH  C_WDAY  C_HOUR  C_VEHS  C_CONF  C_RCFG  C_WTHR  C_RSUR  C_RALN  \\\n",
      "0       7       1       2       2      22       1       3       2       1   \n",
      "1       6       1       4       3      31       1       1       1       1   \n",
      "\n",
      "   C_TRAF  P_SEX  P_AGE  P_PSN  P_USER  P_ISEV  \n",
      "0       7      2      2      1       1       0  \n",
      "1       7      1      4      1       2       1  \n",
      "   C_MNTH  C_WDAY  C_HOUR  C_VEHS  C_CONF  C_RCFG  C_WTHR  C_RSUR  C_RALN  \\\n",
      "0       2       1       2       1       2       4       1       1       2   \n",
      "1       7       7       3       2      21       2       3       2       1   \n",
      "\n",
      "   C_TRAF  P_SEX  P_AGE  P_PSN  P_USER  P_ISEV  \n",
      "0       7      1      4      3       3       1  \n",
      "1       1      2      3      1       1       0  \n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv(file_input, engine = 'python')\n",
    "\n",
    "#load data\n",
    "df_test = pd.read_csv(datafile_test, engine = 'python')\n",
    "df_train = pd.read_csv(datafile_train, engine = 'python')\n",
    "df = df_train.copy()\n",
    "\n",
    "print(df_test.head(2))\n",
    "print(df_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set: (4986688, 15)\n",
      "Test data set: (481616, 15)\n"
     ]
    }
   ],
   "source": [
    "print('Train data set: {}'.format(df_train.shape))\n",
    "print('Test data set: {}'.format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X and y\n",
    "#X = df.iloc[:,0:16]\n",
    "#Y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_test.isnull().sum().sum())\n",
    "print(df_train.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_test[df_test.columns].apply(lambda x: x.astype(str).str.contains('[^0-9]')).sum().sum())\n",
    "print(df_train[df_train.columns].apply(lambda x: x.astype(str).str.contains('[^0-9]')).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 481616 entries, 0 to 481615\n",
      "Data columns (total 15 columns):\n",
      "C_MNTH    481616 non-null category\n",
      "C_WDAY    481616 non-null category\n",
      "C_HOUR    481616 non-null category\n",
      "C_VEHS    481616 non-null category\n",
      "C_CONF    481616 non-null category\n",
      "C_RCFG    481616 non-null category\n",
      "C_WTHR    481616 non-null category\n",
      "C_RSUR    481616 non-null category\n",
      "C_RALN    481616 non-null category\n",
      "C_TRAF    481616 non-null category\n",
      "P_SEX     481616 non-null category\n",
      "P_AGE     481616 non-null category\n",
      "P_PSN     481616 non-null category\n",
      "P_USER    481616 non-null category\n",
      "P_ISEV    481616 non-null int32\n",
      "dtypes: category(14), int32(1)\n",
      "memory usage: 8.3 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4986688 entries, 0 to 4986687\n",
      "Data columns (total 15 columns):\n",
      "C_MNTH    category\n",
      "C_WDAY    category\n",
      "C_HOUR    category\n",
      "C_VEHS    category\n",
      "C_CONF    category\n",
      "C_RCFG    category\n",
      "C_WTHR    category\n",
      "C_RSUR    category\n",
      "C_RALN    category\n",
      "C_TRAF    category\n",
      "P_SEX     category\n",
      "P_AGE     category\n",
      "P_PSN     category\n",
      "P_USER    category\n",
      "P_ISEV    int32\n",
      "dtypes: category(14), int32(1)\n",
      "memory usage: 85.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_test_cat = df_test.astype('int').copy()\n",
    "df_train_cat = df_train.astype('int').copy()\n",
    "\n",
    "# convert to the correct type\n",
    "df_test_cat = convert_type(df_test_cat)\n",
    "print(df_test_cat.info())\n",
    "\n",
    "df_train_cat = convert_type(df_train_cat)\n",
    "print(df_train_cat.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows in test data: 481616\n",
      "Number of Rows in train data: 4986688\n",
      "Total Number of Rows : 5468304\n"
     ]
    }
   ],
   "source": [
    "total_test_Rows = df_test_cat.index.size\n",
    "print(\"Number of Rows in test data: {}\".format(total_test_Rows))\n",
    "\n",
    "total_train_Rows = df_train_cat.index.size\n",
    "print(\"Number of Rows in train data: {}\".format(total_train_Rows))\n",
    "\n",
    "print(\"Total Number of Rows : {}\".format(total_test_Rows + total_train_Rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cat = df_test.astype('int').copy()\n",
    "df_train_cat = df_train.astype('int').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X and y\n",
    "X_train = df_train_cat.iloc[:,0:14]\n",
    "Y_train = df_train_cat.iloc[:,-1]\n",
    "\n",
    "# split data into X and y\n",
    "X_test = df_test_cat.iloc[:,0:14]\n",
    "Y_test = df_test_cat.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split between data and class for training\n",
    "#Y_train = df_train_cat[df_train_cat.columns[-1]]\n",
    "#X_train = df_train_cat[df_train_cat.columns[0:df_train_cat.columns.size -1]]\n",
    "\n",
    "#Y_test = df_test_cat[df_test_cat.columns[-1]]\n",
    "#X_test = df_test_cat[df_test_cat.columns[0:df_test_cat.columns.size -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   C_MNTH  C_WDAY  C_HOUR  C_VEHS  C_CONF  C_RCFG  C_WTHR  C_RSUR  C_RALN  \\\n",
      "0       2       1       2       1       2       4       1       1       2   \n",
      "1       7       7       3       2      21       2       3       2       1   \n",
      "2       6       6       3       3      21       2       1       1       1   \n",
      "3       7       3       5       2      36       3       1       1       1   \n",
      "4       6       2       4       2      21       2       1       1       1   \n",
      "5       8       4       5       2      21       2       1       1       1   \n",
      "6       6       5       2       2      36       2       1       1       2   \n",
      "7       4       1       4       2      21       1       2       1       1   \n",
      "8       7       1       1       2      21       1       1       1       1   \n",
      "9       6       3       1       2      33       2       3       2       2   \n",
      "\n",
      "   C_TRAF  P_SEX  P_AGE  P_PSN  P_USER  \n",
      "0       7      1      4      3       3  \n",
      "1       1      2      3      1       1  \n",
      "2       1      1      5      2       2  \n",
      "3       7      2      3      1       1  \n",
      "4       1      2      4      3       2  \n",
      "5       1      2      4      1       1  \n",
      "6       1      2      4      1       1  \n",
      "7       7      1      3      1       1  \n",
      "8       8      2      4      1       1  \n",
      "9       3      1      3      2       2  \n",
      "   C_MNTH  C_WDAY  C_HOUR  C_VEHS  C_CONF  C_RCFG  C_WTHR  C_RSUR  C_RALN  \\\n",
      "0       7       1       2       2      22       1       3       2       1   \n",
      "1       6       1       4       3      31       1       1       1       1   \n",
      "2       7       4       4       2      21       2       1       1       1   \n",
      "3      12       4       4       2      21       2       2       2       2   \n",
      "4       6       2       2       2      35       2       3       2       1   \n",
      "5       7       1       3       2      21       3       2       1       1   \n",
      "6       1       5       3       2      21       2       1       2       1   \n",
      "7       4       3       2       2      36       2       1       1       1   \n",
      "8       6       3       5       2      31       1       2       1       3   \n",
      "9       2       4       2       2      21       2       1       4       2   \n",
      "\n",
      "   C_TRAF  P_SEX  P_AGE  P_PSN  P_USER  \n",
      "0       7      2      2      1       1  \n",
      "1       7      1      4      1       2  \n",
      "2       1      2      1      2       2  \n",
      "3       7      1      5      1       1  \n",
      "4       7      2      5      1       1  \n",
      "5       7      1      1      1       2  \n",
      "6       1      1      4      1       1  \n",
      "7       1      2      3      1       1  \n",
      "8       7      2      3      1       1  \n",
      "9       7      1      5      1       2  \n"
     ]
    }
   ],
   "source": [
    "print(X_train.head(10))\n",
    "print(X_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 11 17:51:22 2018\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=10, nthread=None, objective='binary:logistic',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=False, subsample=1)\n",
      "[17:51:23] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[17:51:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:51:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:52:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[17:52:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "Tue Dec 11 17:52:33 2018\n"
     ]
    }
   ],
   "source": [
    "# fit model no training data\n",
    "if (enable_model_xgboost):\n",
    "    t_ =  time.time()\n",
    "    print(time.asctime( time.localtime(t_) ))\n",
    "    \n",
    "    nX_train = np.array(X_train)\n",
    "    nY_train = np.array(Y_train)\n",
    "    \n",
    "    #model = xgboost.XGBClassifier(silent=False, learning_rate = 0.3, n_jobs=10)\n",
    "    model = xgboost.XGBClassifier(silent=False, learning_rate = 0.3, n_jobs=10)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    model.fit(nX_train, nY_train)\n",
    "    \n",
    "    # save model to file\n",
    "    pickle.dump(model, open(file_xgboost, \"wb\"))\n",
    "    \n",
    "    t_ =  time.time()\n",
    "    print(time.asctime( time.localtime(t_) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C_MNTH    0\n",
       "C_WDAY    0\n",
       "C_HOUR    0\n",
       "C_VEHS    0\n",
       "C_CONF    0\n",
       "C_RCFG    0\n",
       "C_WTHR    0\n",
       "C_RSUR    0\n",
       "C_RALN    0\n",
       "C_TRAF    0\n",
       "P_SEX     0\n",
       "P_AGE     0\n",
       "P_PSN     0\n",
       "P_USER    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 1 2 ... 2 1 1]\n",
      " [6 1 4 ... 4 1 2]\n",
      " [7 4 4 ... 1 2 2]\n",
      " ...\n",
      " [9 3 3 ... 3 1 1]\n",
      " [6 3 5 ... 3 1 1]\n",
      " [9 1 3 ... 4 1 1]]\n"
     ]
    }
   ],
   "source": [
    "nX_test = np.array(X_test)\n",
    "nY_test = np.array(Y_test)\n",
    "print(nX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGB classifier on train set: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGB classifier on test set: 0.66\n",
      "\n",
      "XGBoost: Confusion Matrix\n",
      "[[173049 103990]\n",
      " [ 57801 146776]]\n",
      "\n",
      "XGBoost: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.75      0.62      0.68    277039\n",
      "          0       0.59      0.72      0.64    204577\n",
      "\n",
      "avg / total       0.68      0.66      0.67    481616\n",
      "\n",
      "\n",
      "Weighted Geometric Mean\n",
      "0.6710129639182502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predictions for test data\n",
    "if (predict_xgboost):\n",
    "    # load model from file\n",
    "    loaded_model = pickle.load(open(file_xgboost, \"rb\"))\n",
    "    \n",
    "    # make predictions for test data\n",
    "    y_pred = loaded_model.predict(nX_test)\n",
    "    \n",
    "    #print(y_pred)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    #accuracy = accuracy_score(nY_test, predictions)\n",
    "    #print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    \n",
    "    print('Accuracy of XGB classifier on train set: {:.2f}'.format(loaded_model.score(nX_train, nY_train)))\n",
    "    print('Accuracy of XGB classifier on test set: {:.2f}'.format(loaded_model.score(nX_test, nY_test)))\n",
    "    \n",
    "    print()\n",
    "    print(\"XGBoost: Confusion Matrix\")\n",
    "    cnf_matrix_lg = confusion_matrix(nY_test, y_pred, labels=labels)\n",
    "    print(cnf_matrix_lg)\n",
    "    print()\n",
    "    print(\"XGBoost: Classification Report\")\n",
    "    print(classification_report(nY_test, y_pred, labels=labels))\n",
    "    print()\n",
    "    print(\"Weighted Geometric Mean\")\n",
    "    gmean = geometric_mean_score(nY_test, y_pred, average='weighted')\n",
    "    print(gmean)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid_mlp = False\n",
    "if (Grid_mlp):\n",
    "    mlp = MLPClassifier(max_iter=100)\n",
    "    \n",
    "    param_grid = {\n",
    "    'hidden_layer_sizes': [(25, 25, 25), (50,50,50), (50,100,50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive']\n",
    "    }\n",
    "    \n",
    "    # Instantiate the grid search model\n",
    "    #grid_search = GridSearchCV(estimator = mlp, param_grid = param_grid, cv = 1, n_jobs = -1, verbose = 3)\n",
    "    grid_search = GridSearchCV(estimator = mlp, param_grid = param_grid, n_jobs = -1, verbose = 3)\n",
    "    \n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    grid_search.best_params_\n",
    "\n",
    "    best_grid = grid_search.best_estimator_\n",
    "    grid_accuracy = evaluate(best_grid, X_test, Y_test)\n",
    "    \n",
    "    print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "RamdomMLP = False\n",
    "if (RamdomMLP):\n",
    "    # Create the random grid\n",
    "    random_grid = {\n",
    "        'hidden_layer_sizes': [(25, 25, 25), (50,50,50)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant','adaptive']\n",
    "    }\n",
    "    print(random_grid)\n",
    "    \n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestClassifier()\n",
    "    \n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    #rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 1, verbose=3, random_state=42, n_jobs = -1)\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 2, verbose=3, random_state=42, n_jobs = -1)\n",
    "    \n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid_RF = False\n",
    "if (Grid_RF):\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [80, 90, 100, 110],\n",
    "        'max_features': [2, 3],\n",
    "        'min_samples_leaf': [3, 4, 5],\n",
    "        'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [100, 200, 300, 1000]\n",
    "        }\n",
    "    # Create a based model\n",
    "    mlp = MLPClassifier(max_iter=100)\n",
    "\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = mlp, param_grid = param_grid, cv = 1, n_jobs = -1, verbose = 3)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    grid_search.best_params_\n",
    "\n",
    "    best_grid = grid_search.best_estimator_\n",
    "    grid_accuracy = evaluate(best_grid, X_test, Y_test)\n",
    "    \n",
    "    print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "HPTune = False\n",
    "if (HPTune):\n",
    "    #from sklearn.model_selection import RandomizedSearchCV\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap\n",
    "                  }\n",
    "    print(random_grid)\n",
    "    \n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestClassifier()\n",
    "    \n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 2, verbose=3, random_state=42, n_jobs = -1)\n",
    "    \n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X_train, Y_train)\n",
    "    ## Find the best parms for random Forest:\n",
    "    best_par = rf_random.best_params_\n",
    "    rf_accuracy = evaluate(best_par, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 481616 entries, 0 to 481615\n",
      "Data columns (total 15 columns):\n",
      "C_MNTH    481616 non-null category\n",
      "C_WDAY    481616 non-null category\n",
      "C_HOUR    481616 non-null category\n",
      "C_VEHS    481616 non-null category\n",
      "C_CONF    481616 non-null category\n",
      "C_RCFG    481616 non-null category\n",
      "C_WTHR    481616 non-null category\n",
      "C_RSUR    481616 non-null category\n",
      "C_RALN    481616 non-null category\n",
      "C_TRAF    481616 non-null category\n",
      "P_SEX     481616 non-null category\n",
      "P_AGE     481616 non-null category\n",
      "P_PSN     481616 non-null category\n",
      "P_USER    481616 non-null category\n",
      "P_ISEV    481616 non-null int32\n",
      "dtypes: category(14), int32(1)\n",
      "memory usage: 8.3 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4986688 entries, 0 to 4986687\n",
      "Data columns (total 15 columns):\n",
      "C_MNTH    category\n",
      "C_WDAY    category\n",
      "C_HOUR    category\n",
      "C_VEHS    category\n",
      "C_CONF    category\n",
      "C_RCFG    category\n",
      "C_WTHR    category\n",
      "C_RSUR    category\n",
      "C_RALN    category\n",
      "C_TRAF    category\n",
      "P_SEX     category\n",
      "P_AGE     category\n",
      "P_PSN     category\n",
      "P_USER    category\n",
      "P_ISEV    int32\n",
      "dtypes: category(14), int32(1)\n",
      "memory usage: 85.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# convert to the correct type\n",
    "df_test_cat = convert_type(df_test_cat)\n",
    "print(df_test_cat.info())\n",
    "\n",
    "df_train_cat = convert_type(df_train_cat)\n",
    "print(df_train_cat.info())\n",
    "# convert to the correct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble (Bagging): Random Forest: Start\n",
      "Tue Dec 11 17:52:39 2018\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=1, warm_start=False)\n",
      "\n",
      "Ensemble (Bagging): Random Forest: Fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 11 18:00:18 2018\n",
      "Ensemble (Bagging): Random Forest: End\n"
     ]
    }
   ],
   "source": [
    "if (enable_model_randomForest):\n",
    "    print(\"Ensemble (Bagging): Random Forest: Start\")\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    forest = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=0, n_jobs=-1, verbose=verbose_level)\n",
    "    print()\n",
    "    print(forest)\n",
    "    \n",
    "    pickle.dump(forest, open(file_random_forest, \"wb\"))\n",
    "    \n",
    "    print()\n",
    "    print(\"Ensemble (Bagging): Random Forest: Fit\")\n",
    "    forest.fit(X_train, Y_train)\n",
    "    \n",
    "    # save model to file\n",
    "    pickle.dump(forest, open(file_random_forest, \"wb\"))\n",
    "    \n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    \n",
    "    print(\"Ensemble (Bagging): Random Forest: End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble (Bagging): Random Forest: Predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    7.6s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RandomForest classifier on train set: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    7.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RandomForest classifier on test set: 0.64\n",
      "Ensemble (Bagging): Random Forest: Confusion Matrix\n",
      "[[176534 100505]\n",
      " [ 74568 130009]]\n",
      "Ensemble (Bagging): Random Forest: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.70      0.64      0.67    277039\n",
      "          0       0.56      0.64      0.60    204577\n",
      "\n",
      "avg / total       0.64      0.64      0.64    481616\n",
      "\n",
      "\n",
      "Weighted Geometric Mean\n",
      "0.6363593318162607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predictions for test data\n",
    "if (predict_randomForest):\n",
    "    \n",
    "    # load model from file\n",
    "    loaded_model = pickle.load(open(file_random_forest, \"rb\"))\n",
    "    \n",
    "    print(\"Ensemble (Bagging): Random Forest: Predict\")\n",
    "    y_pred = forest.predict(X_test)\n",
    "    \n",
    "    print('Accuracy of RandomForest classifier on train set: {:.2f}'.format(forest.score(X_train, Y_train)))\n",
    "    print('Accuracy of RandomForest classifier on test set: {:.2f}'.format(forest.score(X_test, Y_test)))\n",
    "    \n",
    "    print(\"Ensemble (Bagging): Random Forest: Confusion Matrix\")\n",
    "    cnf_matrix_rf = confusion_matrix(Y_test, y_pred, labels=labels)\n",
    "    print(cnf_matrix_rf)\n",
    "    \n",
    "    print(\"Ensemble (Bagging): Random Forest: Classification Report\")\n",
    "    print(classification_report(Y_test,y_pred, labels=labels))\n",
    "    \n",
    "    print()\n",
    "    print(\"Weighted Geometric Mean\")\n",
    "    gmean = geometric_mean_score(Y_test, y_pred, average='weighted')\n",
    "    print(gmean)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer Preceptron: Start\n",
      "Tue Dec 11 18:02:53 2018\n",
      "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(50, 50, 50), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=1, warm_start=False)\n",
      "Multilayer Preceptron: fit\n",
      "Iteration 1, loss = 0.60519821\n",
      "Iteration 2, loss = 0.59852458\n",
      "Iteration 3, loss = 0.59706963\n",
      "Iteration 4, loss = 0.59629244\n",
      "Iteration 5, loss = 0.59566965\n",
      "Iteration 6, loss = 0.59522384\n",
      "Iteration 7, loss = 0.59487311\n",
      "Iteration 8, loss = 0.59464032\n",
      "Iteration 9, loss = 0.59445995\n",
      "Iteration 10, loss = 0.59427484\n",
      "Iteration 11, loss = 0.59414259\n",
      "Iteration 12, loss = 0.59400456\n",
      "Iteration 13, loss = 0.59392907\n",
      "Iteration 14, loss = 0.59381274\n",
      "Iteration 15, loss = 0.59364204\n",
      "Iteration 16, loss = 0.59348303\n",
      "Iteration 17, loss = 0.59335716\n",
      "Iteration 18, loss = 0.59328781\n",
      "Iteration 19, loss = 0.59321556\n",
      "Iteration 20, loss = 0.59315101\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Multilayer Preceptron: Predict\n",
      "Accuracy of Multilayer Perceptron classifier on train set: 0.67\n",
      "Accuracy of Multilayer Perceptron classifier on test set: 0.66\n",
      "\n",
      "Multilayer Preceptron: Confusion Matrix\n",
      "[[173588 103451]\n",
      " [ 58364 146213]]\n",
      "\n",
      "Multilayer Preceptron: Classificiation Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.75      0.63      0.68    277039\n",
      "          0       0.59      0.71      0.64    204577\n",
      "\n",
      "avg / total       0.68      0.66      0.67    481616\n",
      "\n",
      "Tue Dec 11 18:09:26 2018\n",
      "Multilayer Preceptron: End\n"
     ]
    }
   ],
   "source": [
    "if enable_mlp:\n",
    "    print(\"Multilayer Preceptron: Start\")\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    \n",
    "    #mlpc = MLPClassifier(alpha=1)\n",
    "    #mlpc = MLPClassifier(hidden_layer_sizes=(12, 12, 12), max_iter=model_max_iter, verbose=verbose_level)\n",
    "    #mlpc = MLPClassifier(hidden_layer_sizes=(25, 25, 25), verbose=verbose_level, max_iter=model_max_iter)\n",
    "    mlpc = MLPClassifier(activation='tanh', hidden_layer_sizes=(50, 50, 50), verbose=verbose_level, max_iter=model_max_iter, tol = 0.0001)\n",
    "    print(mlpc)\n",
    "    #mlp = multilayer_perceptron(n_hidden =2, activation='logistic', algorithm='sgd', random_state=3)\n",
    "    print(\"Multilayer Preceptron: fit\")\n",
    "    mlpc.fit(X_train, Y_train)\n",
    "    \n",
    "    # save model to file\n",
    "    pickle.dump(mlpc, open(file_mlp, \"wb\"))\n",
    "    \n",
    "if predict_mlp:\n",
    "    \n",
    "    # load model from file\n",
    "    loaded_model = pickle.load(open(file_mlp, \"rb\"))\n",
    "    print(\"Multilayer Preceptron: Predict\")\n",
    "    y_pred = mlpc.predict(X_test)\n",
    "\n",
    "    print('Accuracy of Multilayer Perceptron classifier on train set: {:.2f}'.format(mlpc.score(X_train, Y_train)))\n",
    "    print('Accuracy of Multilayer Perceptron classifier on test set: {:.2f}'.format(mlpc.score(X_test, Y_test)))\n",
    "    print()\n",
    "    print(\"Multilayer Preceptron: Confusion Matrix\")\n",
    "    cnf_matrix_mlp = confusion_matrix(Y_test, y_pred, labels=labels)\n",
    "    print(cnf_matrix_mlp)\n",
    "    print()\n",
    "    print(\"Multilayer Preceptron: Classificiation Report\")\n",
    "    print(classification_report(Y_test,y_pred, labels=labels))\n",
    "    (print)\n",
    "    t_end =  time.time()\n",
    "    print(time.asctime( time.localtime(t_end) ))\n",
    "    \n",
    "    print(\"Multilayer Preceptron: End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 11 18:09:26 2018\n",
      "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(50, 50, 50), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=1, warm_start=False)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=-1, nthread=None, objective='binary:logistic',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=False, subsample=1)\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=1, warm_start=False)\n",
      "Iteration 1, loss = 0.60512454\n",
      "Iteration 2, loss = 0.59815706\n",
      "Iteration 3, loss = 0.59679931\n",
      "Iteration 4, loss = 0.59606023\n",
      "Iteration 5, loss = 0.59560086\n",
      "Iteration 6, loss = 0.59524878\n",
      "Iteration 7, loss = 0.59499954\n",
      "Iteration 8, loss = 0.59470986\n",
      "Iteration 9, loss = 0.59453094\n",
      "Iteration 10, loss = 0.59429916\n",
      "Iteration 11, loss = 0.59417845\n",
      "Iteration 12, loss = 0.59405886\n",
      "Iteration 13, loss = 0.59392270\n",
      "Iteration 14, loss = 0.59378461\n",
      "Iteration 15, loss = 0.59364834\n",
      "Iteration 16, loss = 0.59360561\n",
      "Iteration 17, loss = 0.59354164\n",
      "Iteration 18, loss = 0.59343020\n",
      "Iteration 19, loss = 0.59335744\n",
      "Iteration 20, loss = 0.59329273\n",
      "Iteration 21, loss = 0.59322957\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[18:16:03] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[18:16:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  5.4min finished\n",
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    7.3s finished\n",
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:  1.3min finished\n",
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Voting classifier on train set: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    7.3s finished\n",
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Voting classifier on test set: 0.66\n",
      "\n",
      "Voting: Confusion Matrix\n",
      "[[171583 105456]\n",
      " [ 56178 148399]]\n",
      "\n",
      "Voting: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.75      0.62      0.68    277039\n",
      "          0       0.58      0.73      0.65    204577\n",
      "\n",
      "avg / total       0.68      0.66      0.67    481616\n",
      "\n",
      "\n",
      "Weighted Geometric Mean\n",
      "0.6723228507223615\n",
      "\n",
      "Tue Dec 11 18:24:38 2018\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "voting = True\n",
    "if (voting):\n",
    "    t_ =  time.time()\n",
    "    print(time.asctime( time.localtime(t_) ))\n",
    "    \n",
    "    nX_train = np.array(X_train)\n",
    "    nY_train = np.array(Y_train)\n",
    "    nX_test = np.array(X_test)\n",
    "    nY_test = np.array(Y_test)\n",
    "    \n",
    "    estimators = []\n",
    "    model1 = MLPClassifier(activation='tanh', hidden_layer_sizes=(50, 50, 50), verbose=verbose_level, max_iter=model_max_iter, tol = 0.0001)\n",
    "    estimators.append(('MLP', model1))\n",
    "    model2 = xgboost.XGBClassifier(learning_rate = 0.3, silent=False, n_jobs=-1)\n",
    "    estimators.append(('XGB', model2))\n",
    "    model3 = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=0, n_jobs=-1, verbose=verbose_level)\n",
    "    estimators.append(('RandomForest', model3))\n",
    "    \n",
    "    print(model1)\n",
    "    print(model2)\n",
    "    print(model3)\n",
    "    \n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(estimators)\n",
    "    ensemble.fit(nX_train, nY_train)\n",
    "    y_pred = ensemble.predict(nX_test)\n",
    "    \n",
    "    print('Accuracy of Voting classifier on train set: {:.2f}'.format(ensemble.score(nX_train, nY_train)))\n",
    "    print('Accuracy of Voting classifier on test set: {:.2f}'.format(ensemble.score(nX_test, nY_test)))\n",
    "        \n",
    "    print()\n",
    "    print(\"Voting: Confusion Matrix\")\n",
    "    cnf_matrix_lg = confusion_matrix(nY_test, y_pred, labels=labels)\n",
    "    print(cnf_matrix_lg)\n",
    "    print()\n",
    "    print(\"Voting: Classification Report\")\n",
    "    print(classification_report(nY_test, y_pred, labels=labels))\n",
    "    print()\n",
    "    print(\"Weighted Geometric Mean\")\n",
    "    gmean = geometric_mean_score(nY_test, y_pred, average='weighted')\n",
    "    print(gmean)\n",
    "    print()\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
