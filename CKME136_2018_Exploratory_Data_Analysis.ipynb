{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 600\n",
    "pd.options.display.max_columns=21\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context('paper')\n",
    "#%matplotlib inline\n",
    "\n",
    "import random as rand\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "#from IPython.display import display\n",
    "#display(df)  # OR\n",
    "#print df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datafile to use\n",
    "#Full: NCDB_1999_to_2016.csv\n",
    "#Final: NCDB_FINAL_Cleaned.csv\n",
    "#FullClean: NCDB_FULL_Removed_All_Missing_Values.csv\n",
    "dataSet = \"FullCleanBinary\"\n",
    "if (dataSet == \"FullCleanBinary\"):\n",
    "    datafile = 'NCDB_FULL_Removed_All_Missing_Values_Binary_Class.csv'\n",
    "elif (dataSet == \"FullCleanMulti\"):\n",
    "    datafile = 'NCDB_FULL_Removed_All_Missing_Values_Multi_Class.csv'\n",
    "elif(datafile == \"FinalCleanedBinary\"):\n",
    "    datafile = 'NCDB_FINAL_CleanedBinary.csv'\n",
    "elif(datafile == \"FinalCleanedMulti\"):\n",
    "    datafile = 'NCDB_FINAL_CleanedMulti.csv'\n",
    "else:\n",
    "    datafile = 'NCDB_1999_to_2016.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv(datafile, engine = 'python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Look at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         C_YEAR  C_MNTH  C_WDAY  C_HOUR  C_VEHS  C_CONF  C_RCFG  C_WTHR  \\\n",
      "2437529    2010       4       3      11       4      21       1       1   \n",
      "3553069    2016       6       5       9       2      33       2       1   \n",
      "816860     2002       7       5      19       2      21       2       1   \n",
      "2261683    2009       4       7      13       3      21       1       1   \n",
      "1504391    2005       7       7      22       2      21       1       1   \n",
      "1959322    2007       9       5      19       2      31       2       3   \n",
      "203512     1999      11       7       7       1       6       2       1   \n",
      "2782798    2012       1       7       5       2      41       1       1   \n",
      "2111918    2008       7       2       7       2      21       2       1   \n",
      "899775     2002      11       5      23       1       6       3       4   \n",
      "\n",
      "         C_RSUR  C_RALN  C_TRAF  V_ID  V_TYPE  V_YEAR  P_ID  P_SEX  P_AGE  \\\n",
      "2437529       1       1      18     2       1    2003     2      0     24   \n",
      "3553069       1       1       1     1       1    2010     1      0     63   \n",
      "816860        1       1       1     1       1    1998     2      0      1   \n",
      "2261683       1       2      18     3      11    2008     3      1     14   \n",
      "1504391       1       1      18     1       1    2005     1      1     18   \n",
      "1959322       2       1       1     1       1    1997     2      0     55   \n",
      "203512        1       1      18     1       1    1989     1      1     19   \n",
      "2782798       5       1      18     1       1    2008     1      1     27   \n",
      "2111918       2       1       3     2       1    1993     1      1     34   \n",
      "899775        5       1      18     1       1    1986     1      1     26   \n",
      "\n",
      "         P_PSN  P_SAFE  P_USER  P_ISEV  \n",
      "2437529     13       2       2       2  \n",
      "3553069     11       2       1       1  \n",
      "816860      21       2       2       1  \n",
      "2261683     21       2       2       2  \n",
      "1504391     11       2       1       1  \n",
      "1959322     13       2       2       2  \n",
      "203512      11       2       1       2  \n",
      "2782798     11       2       1       2  \n",
      "2111918     11       2       1       2  \n",
      "899775      11       2       1       2  \n"
     ]
    }
   ],
   "source": [
    "# randomly pick 10 rows from the data set to view\n",
    "rand.seed(101)\n",
    "df_rows = list(range(1, len(df.index)))\n",
    "random_10_rows = rand.sample(df_rows, 10)\n",
    "\n",
    "print(df.loc[random_10_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dimensions of Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 3655334\n",
      "Number of Columns: 21\n",
      "Number of Null values: 0\n",
      "Number of NaN: 0\n",
      "Number of Non Numeric: 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of Rows: {}'.format(df.shape[0]))\n",
    "print('Number of Columns: {}'.format(df.shape[1]))\n",
    "#check for nun numeric values\n",
    "print(\"Number of Null values: {}\".format(df.isnull().sum().sum()))\n",
    "print(\"Number of NaN: {0}\".format(df.isna().sum().sum()))\n",
    "print(\"Number of Non Numeric: {}\".format(df[df.columns].apply(lambda x: x.astype(str).str.contains('[^0-9]')).sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Type For Each Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3655334 entries, 0 to 3655333\n",
      "Data columns (total 21 columns):\n",
      "C_YEAR    category\n",
      "C_MNTH    category\n",
      "C_WDAY    category\n",
      "C_HOUR    category\n",
      "C_VEHS    category\n",
      "C_CONF    category\n",
      "C_RCFG    category\n",
      "C_WTHR    category\n",
      "C_RSUR    category\n",
      "C_RALN    category\n",
      "C_TRAF    category\n",
      "V_ID      category\n",
      "V_TYPE    category\n",
      "V_YEAR    category\n",
      "P_ID      category\n",
      "P_SEX     category\n",
      "P_AGE     int32\n",
      "P_PSN     category\n",
      "P_SAFE    category\n",
      "P_USER    category\n",
      "P_ISEV    category\n",
      "dtypes: category(20), int32(1)\n",
      "memory usage: 83.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = df.astype('category')\n",
    "df['C_YEAR'] = df['C_YEAR'].astype(CategoricalDtype(ordered=True))\n",
    "df['C_MNTH'] = df['C_MNTH'].astype(CategoricalDtype(ordered=True))\n",
    "df['C_WDAY'] = df['C_WDAY'].astype(CategoricalDtype(ordered=True))\n",
    "df['C_HOUR'] = df['C_HOUR'].astype(CategoricalDtype(ordered=True))\n",
    "df['V_YEAR'] = df['V_YEAR'].astype(CategoricalDtype(ordered=True))\n",
    "df['P_AGE'] = df['P_AGE'].astype('int')\n",
    "#print(df.dtypes)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Summary of Categorical Variable(s)\n",
      "         C_YEAR   C_MNTH   C_WDAY   C_HOUR   C_VEHS   C_CONF   C_RCFG   C_WTHR   C_RSUR   C_RALN  \\\n",
      "count   3655334  3655334  3655334  3655334  3655334  3655334  3655334  3655334  3655334  3655334   \n",
      "unique       18       12        7       24       47       18       10        7        9        6   \n",
      "top        2002        8        5       16        2       21        2        1        1        1   \n",
      "freq     237313   341307   622867   330358  2333009  1271029  1932020  2611663  2492364  2851829   \n",
      "\n",
      "         C_TRAF     V_ID   V_TYPE   V_YEAR     P_ID    P_SEX    P_PSN   P_SAFE   P_USER   P_ISEV  \n",
      "count   3655334  3655334  3655334  3655334  3655334  3655334  3655334  3655334  3655334  3655334  \n",
      "unique       17       77       13      111       93        2       12        6        4        2  \n",
      "top          18        1        1     2000        1        1       11        2        1        2  \n",
      "freq    1965540  1990947  3302981   214334  2468055  1966169  2468191  3428824  2420062  2084559  \n",
      "\n",
      "Descriptive Summary of Numeric Variable(s)\n",
      "           P_AGE\n",
      "count  3.655e+06\n",
      "mean   3.656e+01\n",
      "std    1.865e+01\n",
      "min    1.000e+00\n",
      "25%    2.200e+01\n",
      "50%    3.400e+01\n",
      "75%    4.900e+01\n",
      "max    9.900e+01\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('precision', 3)\n",
    "print('Descriptive Summary of Categorical Variable(s)')\n",
    "print(df.describe(exclude=[int]))\n",
    "print()\n",
    "print('Descriptive Summary of Numeric Variable(s)')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution\n",
      "P_ISEV\n",
      "1    1570775\n",
      "2    2084559\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class Distribution\")\n",
    "print(df.groupby('P_ISEV').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Correlation Between Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation\n"
     ]
    }
   ],
   "source": [
    "print('Spearman rank correlation')\n",
    "corr_col = df.columns[0:len(df.columns)-1]\n",
    "print(df[corr_col].astype('int').corr(method='spearman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are really only interested in the ordinal independent variable\n",
    "corr_columns = ['C_YEAR','C_MNTH','C_WDAY','C_HOUR','V_YEAR','P_AGE']\n",
    "print(df[corr_columns].astype('int').corr(method='spearman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Skewness of independent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[corr_col].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['C_YEAR'].value_counts())\n",
    "t1 = df['C_YEAR'].value_counts().sort_index()\n",
    "print(t1.index[0])\n",
    "print(t1.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_levels = pd.DataFrame(columns=['Column', 'Level', 'Count'])\n",
    "df_var = df.columns\n",
    "for col in df_var:\n",
    "    c1 = list()\n",
    "    c2 = list()\n",
    "    c3 = list()\n",
    "    tmp = df[col].value_counts().sort_index()\n",
    "    for i in list(range(0,df[col].nunique())):\n",
    "        c1.append(col)\n",
    "        c2.append(tmp.index[i])\n",
    "        c3.append(tmp.iloc[i])\n",
    "    \n",
    "    df_t2 = pd.DataFrame()\n",
    "    df_t2['Column'] = c1\n",
    "    df_t2['Level'] = c2\n",
    "    df_t2['Count'] = c3\n",
    "    df_levels = pd.concat([df_levels, df_t2])\n",
    "    \n",
    "\n",
    "df_levels['Count'] = df_levels['Count'].astype('int')\n",
    "#display(df_levels)\n",
    "display(pd.pivot_table(df_levels,index=['Column', 'Level']))\n",
    "\n",
    "\n",
    "#>>> data = pd.DataFrame({\"A\": range(3)})\n",
    "#>>> df.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Square Test - Independent vs Dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified code from https://machinelearningmastery.com/chi-squared-test-for-machine-learning/\n",
    "\n",
    "# chi-squared test with similar proportions\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "\n",
    "cat_1 = df['P_ISEV']\n",
    "\n",
    "df_chi_dep_vs_indep = pd.DataFrame(columns=[df.columns[0: len(df.columns) -1]], index = ['P_ISEV'])\n",
    "display(df_chi_dep_vs_indep)\n",
    "\n",
    "for col in df.columns[0: len(df.columns) -1]:\n",
    "    print(col)\n",
    "    \n",
    "    # contingency table\n",
    "    table = np.array([df[col], cat_1])\n",
    "    print(table)\n",
    "    #table_array = np.array(table_series, dtype=pd.Series)\n",
    "    #print(type(table))\n",
    "\n",
    "    stat, p, dof, expected = chi2_contingency(table)\n",
    "    print('dof=%d' % dof)\n",
    "    print(expected)\n",
    "\n",
    "    # interpret test-statistic\n",
    "    prob = 0.95\n",
    "    critical = chi2.ppf(prob, dof)\n",
    "    print('probability=%.3f, critical=%.3f, stat=%.3f' % (prob, critical, stat))\n",
    "    if abs(stat) >= critical:\n",
    "        print('Dependent (reject H0)')\n",
    "        df_chi_dep_vs_indep.loc['P_ISEV',col] = 1  \n",
    "    else:\n",
    "        print('Independent (fail to reject H0)')\n",
    "        df_chi_dep_vs_indep.loc['P_ISEV',col] = 0\n",
    "\n",
    "    # interpret p-value\n",
    "    #alpha = 1.0 - prob\n",
    "    #print('significance=%.3f, p=%.3f' % (alpha, p))\n",
    "    #if p <= alpha:\n",
    "    #    print('Dependent (reject H0)')\n",
    "    #else:\n",
    "    #    print('Independent (fail to reject H0)')\n",
    "display(df_chi_dep_vs_indep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Square Test - compare correlation between independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified code from https://machinelearningmastery.com/chi-squared-test-for-machine-learning/\n",
    "\n",
    "# chi-squared test with similar proportions\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "\n",
    "df_chi = pd.DataFrame(columns=[df.columns[0: len(df.columns) -1]], index = [df.columns[0: len(df.columns) -1]])\n",
    "\n",
    "i=0\n",
    "for c1 in df.columns[0: len(df.columns) -1]:\n",
    "    i = i + 1;\n",
    "    for c2 in df.columns[i: len(df.columns) -1]:\n",
    "        print('c1: {0}, c2: {1}'.format(c1, c2))\n",
    "        # contingency table\n",
    "        table = np.array([df[col], cat_1])\n",
    "\n",
    "        \n",
    "        stat, p, dof, expected = chi2_contingency(table)\n",
    "        print('dof=%d' % dof)\n",
    "        print(expected)\n",
    "\n",
    "        # interpret test-statistic\n",
    "        prob = 0.95\n",
    "        critical = chi2.ppf(prob, dof)\n",
    "        print('probability=%.3f, critical=%.3f, stat=%.3f' % (prob, critical, stat))\n",
    "        if abs(stat) >= critical:\n",
    "            df_chi.loc[c1,c2] = 1\n",
    "            print('Dependent (reject H0)')\n",
    "        else:\n",
    "            print('Independent (fail to reject H0)')\n",
    "            df_chi.loc[c1,c2] = 0\n",
    "\n",
    "display(df_chi)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    g = sns.catplot(col, data=df, kind=\"count\", legend = True, height = 5, aspect = 4)\n",
    "    g.set_xticklabels(step=1)\n",
    "    \n",
    "    h = sns.catplot(col, data=df, kind=\"count\", legend = True, height = 5, aspect = 4, hue = 'P_ISEV')\n",
    "    h.set_xticklabels(step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['P_ISEV'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.boxplot(\"P_ISEV\", \"C_WDAY\", data=df, orient=\"v\")\n",
    "#h = sns.catplot(x=col, data=df, kind=\"box\", orient=\"v\", hue = \"P_ISEV\", legend = True, height = 10, aspect = 0.2)\n",
    "#h.set_xticklabels(step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=df, kind=\"box\", orient=\"v\", height = 10, aspect = 2)\n",
    "g.set_xticklabels(step=1)\n",
    "\n",
    "plt.figure(figsize=(5,10))\n",
    "for col in df.columns:\n",
    "    g = sns.catplot(col, data=df, kind=\"box\", orient=\"v\", height = 10, aspect = 0.25)\n",
    "    g.set_xticklabels(step=1)\n",
    "    \n",
    "    h = sns.catplot(x = 'P_ISEV', y = col, data=df, kind=\"box\", orient=\"v\", height = 15, aspect = 0.6, legend = True)\n",
    "    h.set_xticklabels(step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat Map and Corr Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_col = df.columns[0:len(df.columns)-15]\n",
    "sns.heatmap(df[corr_col].astype('int').corr(method='spearman'),cmap='coolwarm',annot=True, height = 20, linecolor='white',linewidths=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df,hue='P_ISEV',palette='coolwarm', height = 20, linecolor='white',linewidths=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = sns.FacetGrid(df, col=\"C_WDAY\",  row=\"P_ISEV\")\n",
    "k = k.map(plt.hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
