{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 25\n",
    "pd.options.display.max_columns  = 25\n",
    "import time\n",
    "import pickle # allows for model to be saved/load to file\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function converts the data frame to the appropriate data type\n",
    "def convert_type(data):\n",
    "    data = data.astype('category')\n",
    "    data['C_MNTH'] = data['C_MNTH'].astype(CategoricalDtype(ordered=True))\n",
    "    data['C_WDAY'] = data['C_WDAY'].astype(CategoricalDtype(ordered=True))\n",
    "    data['C_HOUR'] = data['C_HOUR'].astype(CategoricalDtype(ordered=True))\n",
    "    data['C_VEHS'] = data['C_VEHS'].astype(CategoricalDtype(ordered=True))\n",
    "    data['P_AGE'] = data['P_AGE'].astype(CategoricalDtype(ordered=True))\n",
    "    data['P_PSN'] = data['P_PSN'].astype(CategoricalDtype(ordered=True))\n",
    "    data['P_ISEV'] = data['P_ISEV'].astype('int')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Algorithm Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Algorithms\n",
    "enable_model_xgboost = True\n",
    "enable_model_randomForest = True\n",
    "enable_multiclass_model = True\n",
    "\n",
    "predict_xgboost = True\n",
    "predict_randomForest = True\n",
    "\n",
    "\n",
    "#Debug\n",
    "verbose_level=1\n",
    "#Multiclass classification, binary if falase\n",
    "multiclass = False\n",
    "over_sample = True\n",
    "\n",
    "if multiclass:\n",
    "    labels=[2, 1, 0]\n",
    "else:\n",
    "    labels=[1, 0]\n",
    "\n",
    "# Datafile\n",
    "#inputfile = 'CKME136X10_2018_Data_CTF.csv'\n",
    "if multiclass:\n",
    "    inputfile_train_O = 'CKME136X10_2018_Data_CTFB_M_O_Train.csv'\n",
    "    inputfile_train_U = 'CKME136X10_2018_Data_CTFB_M_U_Train.csv'\n",
    "    inputfile_test = 'CKME136X10_2018_Data_CTFB_M_Test.csv'\n",
    "else:\n",
    "    inputfile_train_O = 'CKME136X10_2018_Data_CTFB_B_O_Train.csv'\n",
    "    inputfile_train_U = 'CKME136X10_2018_Data_CTFB_B_U_Train.csv'\n",
    "    inputfile_test = 'CKME136X10_2018_Data_CTFB_B_Test.csv'\n",
    "\n",
    "if over_sample:\n",
    "    datafile_train = inputfile_train_O\n",
    "else:\n",
    "    datafile_train = inputfile_train_U\n",
    "\n",
    "datafile_test = inputfile_test\n",
    "\n",
    "\n",
    "#file_input = 'NCDB_FULL_Removed_All_Missing_Values_Binary_Class_Transformed.csv'\n",
    "\n",
    "model_max_iter = 100\n",
    "datestr = 'dec_09_binary_run_1000_BO_ensamble'\n",
    "\n",
    "# Model File Names for storage\n",
    "file_random_forest = 'random_forest_'  + datestr + '.model'\n",
    "file_xgboost = 'xgboost_'  + datestr + '.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(file_input, engine = 'python')\n",
    "\n",
    "#load data\n",
    "df_test = pd.read_csv(datafile_test, engine = 'python')\n",
    "df_train = pd.read_csv(datafile_train, engine = 'python')\n",
    "df = df_train.copy()\n",
    "\n",
    "print(df_test.head(2))\n",
    "print(df_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train data set: {}'.format(df_train.shape))\n",
    "print('Test data set: {}'.format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X and y\n",
    "#X = df.iloc[:,0:16]\n",
    "#Y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.isnull().sum().sum())\n",
    "print(df_train.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test[df_test.columns].apply(lambda x: x.astype(str).str.contains('[^0-9]')).sum().sum())\n",
    "print(df_train[df_train.columns].apply(lambda x: x.astype(str).str.contains('[^0-9]')).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cat = df_test.astype('int').copy()\n",
    "df_train_cat = df_train.astype('int').copy()\n",
    "\n",
    "# convert to the correct type\n",
    "df_test_cat = convert_type(df_test_cat)\n",
    "print(df_test_cat.info())\n",
    "\n",
    "df_train_cat = convert_type(df_train_cat)\n",
    "print(df_train_cat.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_Rows = df_test_cat.index.size\n",
    "print(\"Number of Rows in test data: {}\".format(total_test_Rows))\n",
    "\n",
    "total_train_Rows = df_train_cat.index.size\n",
    "print(\"Number of Rows in train data: {}\".format(total_train_Rows))\n",
    "\n",
    "print(\"Total Number of Rows : {}\".format(total_test_Rows + total_train_Rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cat = df_test.astype('int').copy()\n",
    "df_train_cat = df_train.astype('int').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X and y\n",
    "X_train = df_train_cat.iloc[0:100000,0:14]\n",
    "Y_train = df_train_cat.iloc[:,-1]\n",
    "\n",
    "# split data into X and y\n",
    "X_test = df_test_cat.iloc[:,0:14]\n",
    "Y_test = df_test_cat.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split between data and class for training\n",
    "#Y_train = df_train_cat[df_train_cat.columns[-1]]\n",
    "#X_train = df_train_cat[df_train_cat.columns[0:df_train_cat.columns.size -1]]\n",
    "\n",
    "#Y_test = df_test_cat[df_test_cat.columns[-1]]\n",
    "#X_test = df_test_cat[df_test_cat.columns[0:df_test_cat.columns.size -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.head(10))\n",
    "print(X_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model no training data\n",
    "if (enable_model_xgboost):\n",
    "    t_ =  time.time()\n",
    "    print(time.asctime( time.localtime(t_) ))\n",
    "    \n",
    "    nX_train = np.array(X_train)\n",
    "    nY_train = np.array(Y_train)\n",
    "    \n",
    "    model = xgboost.XGBClassifier(silent=False, n_jobs=10)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    model.fit(nX_train, nY_train)\n",
    "    \n",
    "    # save model to file\n",
    "    pickle.dump(model, open(file_xgboost, \"wb\"))\n",
    "    \n",
    "    t_ =  time.time()\n",
    "    print(time.asctime( time.localtime(t_) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nX_test = np.array(X_test)\n",
    "nY_test = np.array(Y_test)\n",
    "print(nX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions for test data\n",
    "if (predict_xgboost):\n",
    "    # load model from file\n",
    "    loaded_model = pickle.load(open(file_xgboost, \"rb\"))\n",
    "    \n",
    "    # make predictions for test data\n",
    "    y_pred = loaded_model.predict(nX_test)\n",
    "    \n",
    "    #print(y_pred)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    #accuracy = accuracy_score(nY_test, predictions)\n",
    "    #print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    \n",
    "    print('Accuracy of XGB classifier on train set: {:.2f}'.format(ensemble.score(nX_train, nY_train)))\n",
    "    print('Accuracy of XGB classifier on test set: {:.2f}'.format(ensemble.score(nX_test, nY_test)))\n",
    "    \n",
    "    print()\n",
    "    print(\"XGBoost: Confusion Matrix\")\n",
    "    cnf_matrix_lg = confusion_matrix(nY_test, y_pred, labels=labels)\n",
    "    print(cnf_matrix_lg)\n",
    "    print()\n",
    "    print(\"XGBoost: Classification Report\")\n",
    "    print(classification_report(nY_test, y_pred, labels=labels))\n",
    "    print()\n",
    "    print(\"Weighted Geometric Mean\")\n",
    "    gmean = geometric_mean_score(nY_test, y_pred, average='weighted')\n",
    "    print(gmean)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid_mlp = False\n",
    "if (Grid_mlp):\n",
    "    mlp = MLPClassifier(max_iter=100)\n",
    "    \n",
    "    param_grid = {\n",
    "    'hidden_layer_sizes': [(25, 25, 25), (50,50,50), (50,100,50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive']\n",
    "    }\n",
    "    \n",
    "    # Instantiate the grid search model\n",
    "    #grid_search = GridSearchCV(estimator = mlp, param_grid = param_grid, cv = 1, n_jobs = -1, verbose = 3)\n",
    "    grid_search = GridSearchCV(estimator = mlp, param_grid = param_grid, n_jobs = -1, verbose = 3)\n",
    "    \n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    grid_search.best_params_\n",
    "\n",
    "    best_grid = grid_search.best_estimator_\n",
    "    grid_accuracy = evaluate(best_grid, X_test, Y_test)\n",
    "    \n",
    "    print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RamdomMLP = False\n",
    "if (RamdomMLP):\n",
    "    # Create the random grid\n",
    "    random_grid = {\n",
    "        'hidden_layer_sizes': [(25, 25, 25), (50,50,50), (50,100,50)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant','adaptive']\n",
    "    }\n",
    "    print(random_grid)\n",
    "    \n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestClassifier()\n",
    "    \n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    #rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 1, verbose=3, random_state=42, n_jobs = -1)\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, verbose=3, random_state=42, n_jobs = -1)\n",
    "    \n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid_RF = False\n",
    "if (Grid_RF):\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [80, 90, 100, 110],\n",
    "        'max_features': [2, 3],\n",
    "        'min_samples_leaf': [3, 4, 5],\n",
    "        'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [100, 200, 300, 1000]\n",
    "        }\n",
    "    # Create a based model\n",
    "    mlp = MLPClassifier(max_iter=100)\n",
    "\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = mlp, param_grid = param_grid, cv = 1, n_jobs = -1, verbose = 3)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    grid_search.best_params_\n",
    "\n",
    "    best_grid = grid_search.best_estimator_\n",
    "    grid_accuracy = evaluate(best_grid, X_test, Y_test)\n",
    "    \n",
    "    print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "HPTune = False\n",
    "if (HPTune):\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap\n",
    "                  }\n",
    "    print(random_grid)\n",
    "    \n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestClassifier()\n",
    "    \n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 1, verbose=3, random_state=42, n_jobs = -1)\n",
    "    \n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the best parms for random Forest:\n",
    "best_par = rf_random.best_params_\n",
    "rf_accuracy = evaluate(best_par, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to the correct type\n",
    "df_test_cat = convert_type(df_test_cat)\n",
    "print(df_test_cat.info())\n",
    "\n",
    "df_train_cat = convert_type(df_train_cat)\n",
    "print(df_train_cat.info())\n",
    "# convert to the correct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (enable_model_randomForest):\n",
    "    print(\"Ensemble (Bagging): Random Forest: Start\")\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    forest = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=0, n_jobs=10, verbose=verbose_level)\n",
    "    print()\n",
    "    print(forest)\n",
    "    print()\n",
    "    print(\"Ensemble (Bagging): Random Forest: Fit\")\n",
    "    forest.fit(X_train, Y_train)\n",
    "    \n",
    "    # save model to file\n",
    "    pickle.dump(model, open(file_random_forest, \"wb\"))\n",
    "    \n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    \n",
    "    print(\"Ensemble (Bagging): Random Forest: End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions for test data\n",
    "if (predict_randomForest):\n",
    "    \n",
    "    # load model from file\n",
    "    loaded_model = pickle.load(open(file_random_forest, \"rb\"))\n",
    "    \n",
    "    print(\"Ensemble (Bagging): Random Forest: Predict\")\n",
    "    y_pred = forest.predict(X_test)\n",
    "    \n",
    "    print('Accuracy of RandomForest classifier on train set: {:.2f}'.format(forest.score(X_train, Y_train)))\n",
    "    print('Accuracy of RandomForest classifier on test set: {:.2f}'.format(forest.score(X_test, Y_test)))\n",
    "    \n",
    "    print(\"Ensemble (Bagging): Random Forest: Confusion Matrix\")\n",
    "    cnf_matrix_rf = confusion_matrix(Y_test, y_pred, labels=labels)\n",
    "    print(cnf_matrix_rf)\n",
    "    \n",
    "    print(\"Ensemble (Bagging): Random Forest: Classification Report\")\n",
    "    print(classification_report(Y_test,y_pred, labels=labels))\n",
    "    \n",
    "    print()\n",
    "    print(\"Weighted Geometric Mean\")\n",
    "    gmean = geometric_mean_score(Y_test, y_pred, average='weighted')\n",
    "    print(gmean)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Ensemble for Classification\n",
    "voting = True\n",
    "if (voting):\n",
    "    t_ =  time.time()\n",
    "    print(time.asctime( time.localtime(t_) ))\n",
    "    \n",
    "    nX_train = np.array(X_train)\n",
    "    nY_train = np.array(Y_train)\n",
    "    nX_test = np.array(X_test)\n",
    "    nY_test = np.array(Y_test)\n",
    "    \n",
    "    estimators = []\n",
    "    model1 = MLPClassifier(hidden_layer_sizes=(12, 12, 12), verbose=verbose_level, max_iter=model_max_iter, tol = 0.0001)\n",
    "    estimators.append(('MLP', model1))\n",
    "    model2 = xgboost.XGBClassifier(silent=False, n_jobs=10)\n",
    "    estimators.append(('XGB', model2))\n",
    "    model3 = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=0, n_jobs=10, verbose=verbose_level)\n",
    "    estimators.append(('RandomForest', model3))\n",
    "    \n",
    "    print(model1)\n",
    "    print(model2)\n",
    "    print(model3)\n",
    "    \n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(estimators)\n",
    "    ensemble.fit(nX_train, nY_train)\n",
    "    y_pred = ensemble.predict(nX_test)\n",
    "    \n",
    "    print('Accuracy of Voting classifier on train set: {:.2f}'.format(ensemble.score(nX_train, nY_train)))\n",
    "    print('Accuracy of Voting classifier on test set: {:.2f}'.format(ensemble.score(nX_test, nY_test)))\n",
    "        \n",
    "    print()\n",
    "    print(\"XGBoost: Confusion Matrix\")\n",
    "    cnf_matrix_lg = confusion_matrix(nY_test, y_pred, labels=labels)\n",
    "    print(cnf_matrix_lg)\n",
    "    print()\n",
    "    print(\"XGBoost: Classification Report\")\n",
    "    print(classification_report(nY_test, y_pred, labels=labels))\n",
    "    print()\n",
    "    print(\"Weighted Geometric Mean\")\n",
    "    gmean = geometric_mean_score(nY_test, y_pred, average='weighted')\n",
    "    print(gmean)\n",
    "    print()\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
