{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#in case we need to repeat experiment\n",
    "np.random.seed(255)\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 22\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "#Enable\n",
    "enable_grid_search=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('NCDB_2016.csv', engine = 'python')\n",
    "df = pd.read_csv('data01_simple.csv', engine = 'python')\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df[df.index.astype('str').str.contains('[^0-9]')].sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.astype('category').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df.astype('int').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['C_YEAR', 'C_MNTH', 'C_WDAY', 'C_HOUR', 'C_VEHS', 'C_CONF', 'C_RCFG',\n",
      "       'C_WTHR', 'C_RSUR', 'C_RALN', 'C_TRAF', 'V_ID', 'V_TYPE', 'V_YEAR',\n",
      "       'P_ID', 'P_SEX', 'P_AGE', 'P_PSN', 'P_SAFE', 'P_USER', 'P_ISEV'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Class Variable to Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570775\n",
      "2084559\n",
      "['1' '0']\n"
     ]
    }
   ],
   "source": [
    "## Convert Class Variable to Binary\n",
    "### Merge Injury and Fatality as a single class\n",
    "### we will compare the results.\n",
    "df_binary_class = df_cat.copy()\n",
    "\n",
    "#perform the conversion in two steps to avoid any unwanted side effects\n",
    "df_binary_class['P_ISEV'] = df_binary_class['P_ISEV'].map({1: 'safe', 2: 'injury', 3:'fatal'})\n",
    "df_binary_class['P_ISEV'] = df_binary_class['P_ISEV'].map({'safe': '0', 'injury': '1', 'fatal':'1'})\n",
    "print((df_binary_class['P_ISEV']=='0').sum())\n",
    "print((df_binary_class['P_ISEV']=='1').sum())\n",
    "print(df_binary_class['P_ISEV'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100k = df_binary_class.sample(n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training and Testing for Binary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split between data and class\n",
    "Y = df_100k[df_binary_class.columns[-1]]\n",
    "X = df_100k[df_binary_class.columns[0:df_binary_class.columns.size -1]]\n",
    "#print(Xbinary, Ybinary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Test(70%) and Train (30%) for Bianry class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sprint into train and test 70/30\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Xbinary_train, Xbinary_test, Ybinary_train, Ybinary_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write cleaned data to file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets write the datafile for future use\n",
    "df_binary_class.to_csv('cleansimple_binary.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering based on K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, init='random', n_init=10, max_iter=300, tol=1e-04)\n",
    "ykm = kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.00750929e+03, 6.72544020e+00, 3.90723642e+00, 1.36740391e+01,\n",
       "        2.17583745e+00, 2.49713335e+01, 1.75177153e+00, 1.55934615e+00,\n",
       "        1.52826390e+00, 1.34987653e+00, 9.95675864e+00, 1.60760683e+00,\n",
       "        1.81052716e+00, 2.00039521e+03, 1.28245115e+00, 5.38329397e-01,\n",
       "        5.63702222e+01, 1.20277271e+01, 2.26285699e+00, 1.28202169e+00],\n",
       "       [2.00661944e+03, 6.67233440e+00, 4.04238989e+00, 1.41690282e+01,\n",
       "        2.19772110e+00, 3.09138918e+01, 1.94217145e+00, 1.55611642e+00,\n",
       "        1.50366041e+00, 1.29451529e+00, 5.65946393e+00, 1.57905302e+00,\n",
       "        1.54616838e+00, 1.99900992e+03, 1.64972252e+00, 5.19541859e-01,\n",
       "        2.42192998e+01, 1.38276656e+01, 2.26127642e+00, 1.44034124e+00],\n",
       "       [2.00661629e+03, 6.75640848e+00, 4.16547735e+00, 1.33125606e+01,\n",
       "        1.88596370e+00, 1.31032631e+01, 1.39410420e+00, 1.69717334e+00,\n",
       "        1.73881114e+00, 1.58161286e+00, 1.69400028e+01, 1.43265900e+00,\n",
       "        1.78938617e+00, 1.99907517e+03, 1.65363032e+00, 5.57156713e-01,\n",
       "        2.51553623e+01, 1.39008591e+01, 2.33677428e+00, 1.45645698e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ykm.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 2, 0, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ykm.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n",
    "forest.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the inportant features from random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "importFeatures = forest.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the features by importancce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09690289, 0.08829822, 0.06998701, 0.104866  , 0.03198535,\n",
       "       0.07023848, 0.02750522, 0.02843908, 0.02799837, 0.02749222,\n",
       "       0.03320521, 0.0310279 , 0.02038281, 0.1121029 , 0.01870312,\n",
       "       0.02824148, 0.13820599, 0.02022035, 0.01181928, 0.01237812])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16 13  3  0  1  5  2 10  4 11  7 15  8  6  9 12 17 14 19 18]\n",
      "Index(['C_YEAR', 'C_MNTH', 'C_WDAY', 'C_HOUR', 'C_VEHS', 'C_CONF', 'C_RCFG',\n",
      "       'C_WTHR', 'C_RSUR', 'C_RALN', 'C_TRAF', 'V_ID', 'V_TYPE', 'V_YEAR',\n",
      "       'P_ID', 'P_SEX', 'P_AGE', 'P_PSN', 'P_SAFE', 'P_USER'],\n",
      "      dtype='object')\n",
      " 1) P_AGE                          0.138206\n",
      " 2) V_YEAR                         0.112103\n",
      " 3) C_HOUR                         0.104866\n",
      " 4) C_YEAR                         0.096903\n",
      " 5) C_MNTH                         0.088298\n",
      " 6) C_CONF                         0.070238\n",
      " 7) C_WDAY                         0.069987\n",
      " 8) C_TRAF                         0.033205\n",
      " 9) C_VEHS                         0.031985\n",
      "10) V_ID                           0.031028\n",
      "11) C_WTHR                         0.028439\n",
      "12) P_SEX                          0.028241\n",
      "13) C_RSUR                         0.027998\n",
      "14) C_RCFG                         0.027505\n",
      "15) C_RALN                         0.027492\n",
      "16) V_TYPE                         0.020383\n",
      "17) P_PSN                          0.020220\n",
      "18) P_ID                           0.018703\n",
      "19) P_USER                         0.012378\n",
      "20) P_SAFE                         0.011819\n",
      "['P_AGE', 'V_YEAR', 'C_HOUR', 'C_YEAR', 'C_MNTH', 'C_CONF', 'C_WDAY', 'C_TRAF', 'C_VEHS', 'V_ID', 'C_WTHR', 'P_SEX', 'C_RSUR', 'C_RCFG', 'C_RALN', 'V_TYPE', 'P_PSN', 'P_ID', 'P_USER', 'P_SAFE']\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(importFeatures)[::-1]\n",
    "print(indices)\n",
    "featureLabel = X.columns[0:]\n",
    "print(featureLabel)\n",
    "rankedFeature = []\n",
    "for f in range(X.shape[1]):\n",
    "    rankedFeature.append(featureLabel[indices[f]])\n",
    "    print(\"%2d) %-*s %f\" % (f+1, 30,  featureLabel[indices[f]], importFeatures[indices[f]]))\n",
    "print(rankedFeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_AGE</th>\n",
       "      <th>V_YEAR</th>\n",
       "      <th>C_HOUR</th>\n",
       "      <th>C_YEAR</th>\n",
       "      <th>C_MNTH</th>\n",
       "      <th>C_CONF</th>\n",
       "      <th>C_WDAY</th>\n",
       "      <th>C_TRAF</th>\n",
       "      <th>C_VEHS</th>\n",
       "      <th>V_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1497736</th>\n",
       "      <td>9</td>\n",
       "      <td>1993</td>\n",
       "      <td>16</td>\n",
       "      <td>2005</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059749</th>\n",
       "      <td>28</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908179</th>\n",
       "      <td>19</td>\n",
       "      <td>1991</td>\n",
       "      <td>21</td>\n",
       "      <td>2007</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737129</th>\n",
       "      <td>15</td>\n",
       "      <td>1977</td>\n",
       "      <td>18</td>\n",
       "      <td>2002</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345811</th>\n",
       "      <td>32</td>\n",
       "      <td>2000</td>\n",
       "      <td>13</td>\n",
       "      <td>2000</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372130</th>\n",
       "      <td>23</td>\n",
       "      <td>2015</td>\n",
       "      <td>14</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647942</th>\n",
       "      <td>19</td>\n",
       "      <td>2003</td>\n",
       "      <td>18</td>\n",
       "      <td>2011</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760000</th>\n",
       "      <td>12</td>\n",
       "      <td>2003</td>\n",
       "      <td>23</td>\n",
       "      <td>2006</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321151</th>\n",
       "      <td>23</td>\n",
       "      <td>2002</td>\n",
       "      <td>13</td>\n",
       "      <td>2004</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942120</th>\n",
       "      <td>53</td>\n",
       "      <td>2000</td>\n",
       "      <td>14</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425156</th>\n",
       "      <td>68</td>\n",
       "      <td>1999</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340990</th>\n",
       "      <td>21</td>\n",
       "      <td>1998</td>\n",
       "      <td>22</td>\n",
       "      <td>2009</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668869</th>\n",
       "      <td>59</td>\n",
       "      <td>1995</td>\n",
       "      <td>16</td>\n",
       "      <td>2001</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513172</th>\n",
       "      <td>34</td>\n",
       "      <td>2003</td>\n",
       "      <td>11</td>\n",
       "      <td>2005</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140147</th>\n",
       "      <td>21</td>\n",
       "      <td>1988</td>\n",
       "      <td>15</td>\n",
       "      <td>1999</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471561</th>\n",
       "      <td>48</td>\n",
       "      <td>2004</td>\n",
       "      <td>13</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469089</th>\n",
       "      <td>69</td>\n",
       "      <td>2000</td>\n",
       "      <td>14</td>\n",
       "      <td>2010</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924416</th>\n",
       "      <td>20</td>\n",
       "      <td>2003</td>\n",
       "      <td>21</td>\n",
       "      <td>2007</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486421</th>\n",
       "      <td>24</td>\n",
       "      <td>2002</td>\n",
       "      <td>22</td>\n",
       "      <td>2010</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514255</th>\n",
       "      <td>20</td>\n",
       "      <td>1994</td>\n",
       "      <td>14</td>\n",
       "      <td>2005</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318244</th>\n",
       "      <td>65</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884976</th>\n",
       "      <td>33</td>\n",
       "      <td>1993</td>\n",
       "      <td>20</td>\n",
       "      <td>2007</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        P_AGE V_YEAR C_HOUR C_YEAR C_MNTH C_CONF C_WDAY C_TRAF C_VEHS V_ID\n",
       "1497736     9   1993     16   2005      7     21      5      3      2    2\n",
       "3059749    28   2012      8   2013      8     36      5     18      2    1\n",
       "1908179    19   1991     21   2007      6     31      6      1      3    3\n",
       "737129     15   1977     18   2002      3     31      4      1      2    2\n",
       "345811     32   2000     13   2000      7     31      4      3      2    2\n",
       "3372130    23   2015     14   2015      6     33      2      1      3    1\n",
       "2647942    19   2003     18   2011      5     35      5     18      2    1\n",
       "1760000    12   2003     23   2006     10      6      1     18      1    1\n",
       "1321151    23   2002     13   2004      9     31      6     18      2    2\n",
       "942120     53   2000     14   2003      1     21      4     18      3    2\n",
       "3425156    68   1999     10   2015      9     35      5      2      2    1\n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...    ...  ...\n",
       "2340990    21   1998     22   2009      9     33      6      1      2    1\n",
       "668869     59   1995     16   2001     11     21      6      1      4    1\n",
       "1513172    34   2003     11   2005      8     21      3     18      3    2\n",
       "140147     21   1988     15   1999      8     21      5     18      3    3\n",
       "3471561    48   2004     13   2015     12     35      4      3      1    1\n",
       "2469089    69   2000     14   2010      6     21      3     18      3    2\n",
       "1924416    20   2003     21   2007      7     21      6     18      3    2\n",
       "2486421    24   2002     22   2010      7      6      3     18      1    1\n",
       "1514255    20   1994     14   2005      8     23      4     18      2    1\n",
       "3318244    65   2014      9   2015      2     21      3      1      2    2\n",
       "1884976    33   1993     20   2007      5      4      4     18      1    1\n",
       "\n",
       "[100000 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#select features that contribute more than 0.05\n",
    "#[df_cat.columns[0:df_cat.columns.size -1]]\n",
    "X_Selected = X[rankedFeature[0:10]]\n",
    "display(X_Selected)\n",
    "X_Selected.shape\n",
    "\n",
    "#sprint into train and test 70/30\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_Selected, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM GridSearch for Optimal Parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This operation is computationaly expensive.\n",
    "#Enable as required.\n",
    "enable_grid_search = False\n",
    "if enable_grid_search:\n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "    from sklearn.svm import SVC\n",
    "    param_grid = {'C':[0.1, 1, 10, 100, 1000], 'gamma':[1, 0.1, 0.01, 0.001, 0.0001]}\n",
    "    grid = GridSearchCV(SVC(), param_grid, verbose=3)\n",
    "    grid.fit(X_train, Y_train)\n",
    "    print(grid.best_params_)\n",
    "    #{'C': 1000, 'gamma': 0.001}\n",
    "    print(grid.best_estimator_)\n",
    "    #SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    #  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
    "    #  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    #  tol=0.001, verbose=False)\n",
    "    grid_predictions = grid.predict(X_test)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    confusion_matrix = confusion_matrix(Y_test, grid_predictions)\n",
    "    print(confusion_matrix)\n",
    "    print(classification_report(Y_test,grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1, random_state=0)\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on train set: 0.59\n",
      "Accuracy of logistic regression classifier on test set: 0.59\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "#display(y_pred)\n",
    "print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(lr.score(X_train, Y_train)))\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(lr.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00110126])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the intercept (Note: one vs rest => 1 vs 2and3, 2 vs 1and3, 3 vs 1and2)\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00606785, -0.01332486, -0.01725665,  0.01364966,  0.00104168,\n",
       "        -0.00519001,  0.00182394,  0.01871573, -0.37912878,  0.17243438]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the coeficients (Note: one vs rest => 1 vs 2and3, 2 vs 1and3, 3 vs 1and2)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3187  9576]\n",
      " [ 2730 14507]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.25      0.34     12763\n",
      "          1       0.60      0.84      0.70     17237\n",
      "\n",
      "avg / total       0.58      0.59      0.55     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with L1 regularization\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty='l1', C=1000, random_state=0)\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on train set: 0.59\n",
      "Accuracy of logistic regression classifier on test set: 0.59\n",
      "[[ 3134  9629]\n",
      " [ 2658 14579]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.25      0.34     12763\n",
      "          1       0.60      0.85      0.70     17237\n",
      "\n",
      "avg / total       0.58      0.59      0.55     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(lr.score(X_train, Y_train)))\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(lr.score(X_test, Y_test)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decison Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(criterion='entropy',max_depth=5, random_state=0)\n",
    "tree.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on train set: 0.59\n",
      "Accuracy of Tree classifier on test set: 0.64\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(lr.score(X_train, Y_train)))\n",
    "print('Accuracy of Tree classifier on test set: {:.2f}'.format(tree.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5438  7325]\n",
      " [ 3560 13677]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.43      0.50     12763\n",
      "          1       0.65      0.79      0.72     17237\n",
      "\n",
      "avg / total       0.63      0.64      0.62     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=2,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(criterion='entropy', n_estimators=1000, random_state=0, n_jobs=2)\n",
    "forest.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on train set: 0.59\n",
      "Accuracy of RandomForest classifier on test set: 0.63\n"
     ]
    }
   ],
   "source": [
    "y_pred = forest.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(lr.score(X_train, Y_train)))\n",
    "print('Accuracy of RandomForest classifier on test set: {:.2f}'.format(forest.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6070  6693]\n",
      " [ 4394 12843]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.48      0.52     12763\n",
      "          1       0.66      0.75      0.70     17237\n",
      "\n",
      "avg / total       0.62      0.63      0.62     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-N-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on train set: 0.59\n",
      "Accuracy of RandomForest classifier on test set: 0.63\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(lr.score(X_train, Y_train)))\n",
    "print('Accuracy of RandomForest classifier on test set: {:.2f}'.format(forest.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5555  7208]\n",
      " [ 5622 11615]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.44      0.46     12763\n",
      "          1       0.62      0.67      0.64     17237\n",
      "\n",
      "avg / total       0.57      0.57      0.57     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "#svm = SVC(C=1, random_state=0, kernel='sigmoid', verbose=True)\n",
    "#svm = SVC(C=1, random_state=0, kernel='linear', verbose=True, cache_size=200)\n",
    "svm = SVC(verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on train set: 0.59\n",
      "Accuracy of SVM regression classifier on test set: 0.58\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(lr.score(X_train, Y_train)))\n",
    "print('Accuracy of SVM regression classifier on test set: {:.2f}'.format(svm.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3892  8871]\n",
      " [ 3754 13483]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.30      0.38     12763\n",
      "          1       0.60      0.78      0.68     17237\n",
      "\n",
      "avg / total       0.56      0.58      0.55     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Tuning using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_grid_search = False\n",
    "if enable_grid_search:\n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "    from sklearn.svm import SVC\n",
    "    param_grid = {'C':[0.1, 1, 10, 100, 1000], 'gamma':[1, 0.1, 0.01, 0.001, 0.0001]}\n",
    "    grid = GridSearchCV(SVC(), param_grid, verbose=3)\n",
    "    grid.fit(X_train, Y_train)\n",
    "    grid.best_params_\n",
    "    #Result: {'C': 1000, 'gamma': 0.001}\n",
    "    grid.best_estimator_\n",
    "        #SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
    "        #decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
    "        #max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "        #tol=0.001, verbose=False)\n",
    "    grid_predictions = grid.predict(X_test)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    confusion_matrix = confusion_matrix(Y_test, grid_predictions)\n",
    "    print(confusion_matrix)\n",
    "    print(classification_report(Y_test,grid_predictions))\n",
    "    \n",
    "    \n",
    "#[[4810 1501  697]\n",
    "# [3549 2112 1362]\n",
    "# [ 801  905 5036]]\n",
    "#             precision    recall  f1-score   support\n",
    "#\n",
    "#          1       0.53      0.69      0.60      7008\n",
    "#          2       0.47      0.30      0.37      7023\n",
    "#          3       0.71      0.75      0.73      6742\n",
    "#\n",
    "#avg / total       0.57      0.58      0.56     20773"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN - Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#mlpc = MLPClassifier(alpha=1)\n",
    "mlpc = MLPClassifier(hidden_layer_sizes=(12, 12, 12), max_iter=1000)\n",
    "\n",
    "#mlp = multilayer_perceptron(n_hidden =2, activation='logistic', algorithm='sgd', random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(12, 12, 12), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlpc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3636  9127]\n",
      " [ 2886 14351]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.28      0.38     12763\n",
      "          1       0.61      0.83      0.70     17237\n",
      "\n",
      "avg / total       0.59      0.60      0.57     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check sigmoid and rbf\n",
    "#from sklearn.ensemble import BaggingClassifier\n",
    "#from sklearn.svm import SVC\n",
    "#clf = BaggingClassifier(SVC(C=1.0,\n",
    "#        cache_size=200,\n",
    "#        class_weight=None,\n",
    "#        coef0=0.0,\n",
    "#        decision_function_shape=None,\n",
    "#        degree=3,\n",
    "#        gamma='auto',\n",
    "#        kernel='linear',\n",
    "#        max_iter=-1,\n",
    "#        probability=False,\n",
    "#        random_state=None,\n",
    "#        shrinking=True,\n",
    "#        tol=0.001,\n",
    "#        verbose=False,\n",
    "#        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
