{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#in case we need to repeat experiment\n",
    "#np.random.seed(255)\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 22\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pickle # allows for model to be saved/load to file\n",
    "import time\n",
    "\n",
    "#Use print instead of display when run as python script\n",
    "pyscript = True\n",
    "\n",
    "#Classifier verborsity where supported\n",
    "verbose_level=3\n",
    "\n",
    "#sampleN = 4300000\n",
    "\n",
    "#Multiclass classification, binary if falase\n",
    "multiclass = False\n",
    "over_sample = True\n",
    "\n",
    "#inputfile = 'CKME136X10_2018_Data_CTF.csv'\n",
    "if multiclass:\n",
    "    inputfile_train_O = 'CKME136X10_2018_Data_CTFB_M_O_Train.csv'\n",
    "    inputfile_train_U = 'CKME136X10_2018_Data_CTFB_M_U_Train.csv'\n",
    "    inputfile_test = 'CKME136X10_2018_Data_CTFB_M_Test.csv'\n",
    "else:\n",
    "    inputfile_train_O = 'CKME136X10_2018_Data_CTFB_B_O_Train.csv'\n",
    "    inputfile_train_U = 'CKME136X10_2018_Data_CTFB_B_U_Train.csv'\n",
    "    inputfile_test = 'CKME136X10_2018_Data_CTFB_B_Test.csv'\n",
    "\n",
    "if over_sample:\n",
    "    datafile_train = inputfile_train_O\n",
    "else:\n",
    "    datafile_train = inputfile_train_U\n",
    "\n",
    "datafile_test = inputfile_test\n",
    "    \n",
    "model_max_iter = 1000\n",
    "datestr = 'dec_04_binary_run_1000_BO'\n",
    "\n",
    "#Model Store\n",
    "file_lr = 'lr_' + datestr + '.model'\n",
    "file_lr_l1 = 'lr_l2_' + datestr + '.model'\n",
    "file_dt = 'dt_' + datestr + '.model'\n",
    "file_svm = 'svm_' + datestr + '.model'\n",
    "file_knn = 'knn_' + datestr + '.model'\n",
    "file_mlp = 'mlp_' + datestr + '.model'\n",
    "file_kmean = 'kmean_' + datestr + '.model'\n",
    "file_nbayes = 'nbayes_' + datestr + '.model'\n",
    "\n",
    "file_final_train = 'final_train_' + datestr + '.csv'\n",
    "file_final_test = 'final_test_' + datestr + '.csv'\n",
    "\n",
    "#Enable Optimization Algorithms\n",
    "enable_grid_search = False\n",
    "svm_c = 1\n",
    "svm_gamma = 1\n",
    "feature_all = True\n",
    "defaultFeatures = ['P_AGE', 'V_YEAR', 'C_HOUR', 'C_YEAR', 'C_MNTH', 'C_CONF', 'C_WDAY', 'C_VEHS', 'P_USER', 'P_SEX']\n",
    "\n",
    "enable_lr_l1 = True\n",
    "predict_lr_l1 = True\n",
    "\n",
    "# Enable Algorithms\n",
    "enable_lr = True\n",
    "enable_dt = True\n",
    "enable_svm = True\n",
    "enable_knn = True\n",
    "enable_mlp = True\n",
    "enable_kmean = True\n",
    "enable_nbayes = True\n",
    "\n",
    "predict_lr = True\n",
    "predict_dt = True\n",
    "predict_svm = True\n",
    "predict_knn = True\n",
    "predict_mlp = True\n",
    "predict_nbayes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Class Classification: Disabled\n",
      "Grid Search: Disabled\n",
      "All Features: Enabled\n",
      "K-means: Enabled\n",
      "Logistic Regression: Enabled\n",
      "Decision Tree: Enabled\n",
      "Support Vector Machines: Enabled\n",
      "KNN: Enabled\n",
      "MLP: Enabled\n"
     ]
    }
   ],
   "source": [
    "#print(\"Sample size: {}\".format(sampleN))\n",
    "\n",
    "if multiclass:\n",
    "    print(\"Multi-Class Classification: Enabled\")\n",
    "else:\n",
    "    print(\"Multi-Class Classification: Disabled\")\n",
    "\n",
    "if enable_grid_search:\n",
    "    print(\"Grid Search: Enabled\")\n",
    "else:\n",
    "    print(\"Grid Search: Disabled\")\n",
    "\n",
    "if feature_all:\n",
    "    print(\"All Features: Enabled\")\n",
    "else:\n",
    "    print(\"All Features: Disabled\")\n",
    "    \n",
    "if enable_kmean:\n",
    "    print(\"K-means: Enabled\")\n",
    "else:\n",
    "    print(\"K-means: Disabled\")\n",
    "\n",
    "if enable_lr_l1:\n",
    "    print(\"Logistic Regression: Enabled\")\n",
    "else:\n",
    "    print(\"Logistic Regression: Disabled\")\n",
    "    \n",
    "if enable_dt:\n",
    "    print(\"Decision Tree: Enabled\")\n",
    "else:\n",
    "    print(\"Decision Tree: Disabled\")\n",
    "    \n",
    "if enable_svm:\n",
    "    print(\"Support Vector Machines: Enabled\")\n",
    "else:\n",
    "    print(\"Support Vector Machines: Disabled\")\n",
    "\n",
    "if  enable_knn:\n",
    "    print(\"KNN: Enabled\")\n",
    "else:\n",
    "    print(\"KNN: Disabled\")\n",
    "    \n",
    "if enable_mlp:\n",
    "    print(\"MLP: Enabled\")\n",
    "else:\n",
    "    print(\"MLP: Disabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  3 01:16:12 2018\n"
     ]
    }
   ],
   "source": [
    "t_start =  time.time()\n",
    "print(time.asctime( time.localtime(t_start) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   C_YEAR  C_MNTH  C_WDAY  C_HOUR  C_VEHS  C_CONF  C_RCFG  C_WTHR  C_RSUR  \\\n",
      "0       1       5       5       2       3      21       2       1       1   \n",
      "1      14       2       6       1       4      24       1       1       1   \n",
      "\n",
      "   C_RALN  C_TRAF  V_TYPE  V_YEAR  P_SEX  P_AGE  P_PSN  P_SAFE  P_USER  P_ISEV  \n",
      "0       1       1       6       2      1     34      1       2       1       0  \n",
      "1       2       7       1       4      0     47      3       2       2       1  \n",
      "   C_YEAR  C_MNTH  C_WDAY  C_HOUR  C_VEHS  C_CONF  C_RCFG  C_WTHR  C_RSUR  \\\n",
      "0       3       5       4       3       2      51       1       1       1   \n",
      "1      16       3       1       1       2      51       1       1       1   \n",
      "\n",
      "   C_RALN  C_TRAF  V_TYPE  V_YEAR  P_SEX  P_AGE  P_PSN  P_SAFE  P_USER  P_ISEV  \n",
      "0       1       7       1       3      1     18      1       2       2       1  \n",
      "1       1       7       6       4      1     19      1       2       1       1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3534154 entries, 0 to 3534153\n",
      "Data columns (total 19 columns):\n",
      "C_YEAR    int64\n",
      "C_MNTH    int64\n",
      "C_WDAY    int64\n",
      "C_HOUR    int64\n",
      "C_VEHS    int64\n",
      "C_CONF    int64\n",
      "C_RCFG    int64\n",
      "C_WTHR    int64\n",
      "C_RSUR    int64\n",
      "C_RALN    int64\n",
      "C_TRAF    int64\n",
      "V_TYPE    int64\n",
      "V_YEAR    int64\n",
      "P_SEX     int64\n",
      "P_AGE     int64\n",
      "P_PSN     int64\n",
      "P_SAFE    int64\n",
      "P_USER    int64\n",
      "P_ISEV    int64\n",
      "dtypes: int64(19)\n",
      "memory usage: 512.3 MB\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "df_test = pd.read_csv(datafile_test, engine = 'python')\n",
    "df_train = pd.read_csv(datafile_train, engine = 'python')\n",
    "df = df_train.copy()\n",
    "\n",
    "print(df_test.head(2))\n",
    "print(df_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_test.isnull().sum().sum())\n",
    "print(df_train.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_test[df_test.columns].apply(lambda x: x.astype(str).str.contains('[^0-9]')).sum().sum())\n",
    "print(df_train[df_train.columns].apply(lambda x: x.astype(str).str.contains('[^0-9]')).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cat = df_test.astype('category').copy()\n",
    "df_train_cat = df_train.astype('category').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows in test data: 1300968\n",
      "Number of Rows in train data: 3534154\n"
     ]
    }
   ],
   "source": [
    "total_test_Rows = df_test_cat.index.size\n",
    "print(\"Number of Rows in test data: {}\".format(total_test_Rows))\n",
    "\n",
    "total_train_Rows = df_train_cat.index.size\n",
    "print(\"Number of Rows in train data: {}\".format(total_train_Rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['C_YEAR', 'C_MNTH', 'C_WDAY', 'C_HOUR', 'C_VEHS', 'C_CONF', 'C_RCFG',\n",
      "       'C_WTHR', 'C_RSUR', 'C_RALN', 'C_TRAF', 'V_TYPE', 'V_YEAR', 'P_SEX',\n",
      "       'P_AGE', 'P_PSN', 'P_SAFE', 'P_USER', 'P_ISEV'],\n",
      "      dtype='object')\n",
      "C_YEAR    category\n",
      "C_MNTH    category\n",
      "C_WDAY    category\n",
      "C_HOUR    category\n",
      "C_VEHS    category\n",
      "C_CONF    category\n",
      "C_RCFG    category\n",
      "C_WTHR    category\n",
      "C_RSUR    category\n",
      "C_RALN    category\n",
      "C_TRAF    category\n",
      "V_TYPE    category\n",
      "V_YEAR    category\n",
      "P_SEX     category\n",
      "P_AGE     category\n",
      "P_PSN     category\n",
      "P_SAFE    category\n",
      "P_USER    category\n",
      "P_ISEV    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train_cat.columns)\n",
    "print(df_train_cat.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot-Encoding of categorical\n",
    "#TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#print(df_test_cat[df_test_cat.columns].apply(lambda x: x.astype(str).str.contains('[^0-9]')).sum().sum())\n",
    "#print(df_train_cat[df_train_cat.columns].apply(lambda x: x.astype(str).str.contains('[^0-9]')).sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type cast train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3534154 entries, 0 to 3534153\n",
      "Data columns (total 19 columns):\n",
      "C_YEAR    category\n",
      "C_MNTH    category\n",
      "C_WDAY    category\n",
      "C_HOUR    category\n",
      "C_VEHS    int32\n",
      "C_CONF    category\n",
      "C_RCFG    category\n",
      "C_WTHR    category\n",
      "C_RSUR    category\n",
      "C_RALN    category\n",
      "C_TRAF    category\n",
      "V_TYPE    category\n",
      "V_YEAR    category\n",
      "P_SEX     category\n",
      "P_AGE     int32\n",
      "P_PSN     category\n",
      "P_SAFE    category\n",
      "P_USER    category\n",
      "P_ISEV    int32\n",
      "dtypes: category(16), int32(3)\n",
      "memory usage: 94.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# convert to the correct type\n",
    "df_train_cat['C_YEAR'] = df_train_cat['C_YEAR'].astype(CategoricalDtype(ordered=True))\n",
    "df_train_cat['C_MNTH'] = df_train_cat['C_MNTH'].astype(CategoricalDtype(ordered=True))\n",
    "df_train_cat['C_WDAY'] = df_train_cat['C_WDAY'].astype(CategoricalDtype(ordered=True))\n",
    "df_train_cat['C_HOUR'] = df_train_cat['C_HOUR'].astype(CategoricalDtype(ordered=True))\n",
    "df_train_cat['C_VEHS'] = df_train_cat['C_VEHS'].astype('int')\n",
    "df_train_cat['V_YEAR'] = df_train_cat['V_YEAR'].astype(CategoricalDtype(ordered=True))\n",
    "df_train_cat['P_PSN'] = df_train_cat['P_PSN'].astype(CategoricalDtype(ordered=True))\n",
    "df_train_cat['P_AGE'] = df_train_cat['P_AGE'].astype('int')\n",
    "df_train_cat['P_ISEV'] = df_train_cat['P_ISEV'].astype('int')\n",
    "\n",
    "print(df_train_cat.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type cast test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1300968 entries, 0 to 1300967\n",
      "Data columns (total 19 columns):\n",
      "C_YEAR    1300968 non-null category\n",
      "C_MNTH    1300968 non-null category\n",
      "C_WDAY    1300968 non-null category\n",
      "C_HOUR    1300968 non-null category\n",
      "C_VEHS    1300968 non-null int32\n",
      "C_CONF    1300968 non-null category\n",
      "C_RCFG    1300968 non-null category\n",
      "C_WTHR    1300968 non-null category\n",
      "C_RSUR    1300968 non-null category\n",
      "C_RALN    1300968 non-null category\n",
      "C_TRAF    1300968 non-null category\n",
      "V_TYPE    1300968 non-null category\n",
      "V_YEAR    1300968 non-null category\n",
      "P_SEX     1300968 non-null category\n",
      "P_AGE     1300968 non-null int32\n",
      "P_PSN     1300968 non-null category\n",
      "P_SAFE    1300968 non-null category\n",
      "P_USER    1300968 non-null category\n",
      "P_ISEV    1300968 non-null int32\n",
      "dtypes: category(16), int32(3)\n",
      "memory usage: 34.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# convert to the correct type\n",
    "df_test_cat['C_YEAR'] = df_test_cat['C_YEAR'].astype(CategoricalDtype(ordered=True))\n",
    "df_test_cat['C_MNTH'] = df_test_cat['C_MNTH'].astype(CategoricalDtype(ordered=True))\n",
    "df_test_cat['C_WDAY'] = df_test_cat['C_WDAY'].astype(CategoricalDtype(ordered=True))\n",
    "df_test_cat['C_HOUR'] = df_test_cat['C_HOUR'].astype(CategoricalDtype(ordered=True))\n",
    "df_test_cat['C_VEHS'] = df_test_cat['C_VEHS'].astype('int')\n",
    "df_test_cat['V_YEAR'] = df_test_cat['V_YEAR'].astype(CategoricalDtype(ordered=True))\n",
    "df_test_cat['P_PSN'] = df_test_cat['P_PSN'].astype(CategoricalDtype(ordered=True))\n",
    "df_test_cat['P_AGE'] = df_test_cat['P_AGE'].astype('int')\n",
    "df_test_cat['P_ISEV'] = df_test_cat['P_ISEV'].astype('int')\n",
    "print(df_test_cat.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training and Testing for Binary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split between data and class for training\n",
    "Y_train = df_train_cat[df_train_cat.columns[-1]]\n",
    "X_train = df_train_cat[df_train_cat.columns[0:df_train_cat.columns.size -1]]\n",
    "\n",
    "Y_test = df_test_cat[df_test_cat.columns[-1]]\n",
    "X_test = df_test_cat[df_test_cat.columns[0:df_test_cat.columns.size -1]]\n",
    "\n",
    "# split data into X and y\n",
    "#X = df_sample.iloc[:,0:16]\n",
    "#Y = df_sample.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C_YEAR C_MNTH C_WDAY C_HOUR  C_VEHS C_CONF C_RCFG C_WTHR C_RSUR C_RALN  \\\n",
      "0      3      5      4      3       2     51      1      1      1      1   \n",
      "1     16      3      1      1       2     51      1      1      1      1   \n",
      "2     14      6      2      3       1      6      1      1      5      3   \n",
      "\n",
      "  C_TRAF V_TYPE V_YEAR P_SEX  P_AGE P_PSN P_SAFE P_USER  \n",
      "0      7      1      3     1     18     1      2      2  \n",
      "1      7      6      4     1     19     1      2      1  \n",
      "2      7     14      4     1     29     1      3      3  \n"
     ]
    }
   ],
   "source": [
    "print(X_train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering based on K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_kmean:\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    print(\"K-Means Clustering: Start\")\n",
    "    kmeans = KMeans(n_clusters=3, init='random', n_init=10, tol=1e-04, verbose= verbose_level, max_iter=model_max_iter)\n",
    "    print(kmeans)\n",
    "    \n",
    "    print(\"K-Means Clustering: Build\")\n",
    "    ykm = kmeans.fit(X_train)\n",
    "    \n",
    "    if pyscript:\n",
    "        print(ykm.cluster_centers_)\n",
    "        print(ykm.labels_)\n",
    "    else:\n",
    "        display(ykm.cluster_centers_)\n",
    "        display(ykm.labels_)\n",
    "    \n",
    "    # save model to file\n",
    "    pickle.dump(ykm, open(file_kmean, \"wb\"))\n",
    "    \n",
    "    print(\"K-Means Clustering: End\")\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM GridSearch for Optimal Parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This operation is computationaly expensive.\n",
    "if enable_grid_search:\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    param_grid = {'C':[0.1, 1, 10, 100, 1000], 'gamma':[1, 0.1, 0.01, 0.001, 0.0001]}\n",
    "    grid = GridSearchCV(SVC(), param_grid, verbose=verbose_level, n_jobs = 10)\n",
    "    print(grid)\n",
    "    grid.fit(X_train, Y_train)\n",
    "    print(grid.best_params_)\n",
    "    svm_c = grid.best_params_.get('C')\n",
    "    svm_gamma = grid.best_params_.get('gamma')\n",
    "    print(grid.best_estimator_)\n",
    "    grid_predictions = grid.predict(X_test)\n",
    "    cfn_matrix_grid = confusion_matrix(Y_test, grid_predictions)\n",
    "    print(cfn_matrix_grid)\n",
    "    print(classification_report(Y_test,grid_predictions))\n",
    "    t_end =  time.time()\n",
    "    print(time.asctime( time.localtime(t_end) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Start\n",
      "Mon Dec  3 01:47:17 2018\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=10,\n",
      "          penalty='l2', random_state=0, solver='saga', tol=0.0001,\n",
      "          verbose=3, warm_start=False)\n",
      "Logistic Regression: Fit\n",
      "convergence after 14 epochs took 34 seconds\n",
      "Logistic Regression: Predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   1 out of   1 | elapsed:   33.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on train set: 0.61\n",
      "Accuracy of logistic regression classifier on test set: 0.61\n",
      "Logistic Regression: Intercept\n",
      "[0.29383974]\n",
      "Logistic Regression: Coefficients\n",
      "[[ 8.78294161e-03  6.31354603e-03  9.96373316e-03 -2.60569677e-02\n",
      "  -3.31034047e-01 -2.97111836e-03 -4.10734920e-04  2.78606644e-02\n",
      "   9.52500831e-02  2.41676189e-01  4.18425213e-02  7.60565306e-02\n",
      "  -1.32634835e-01 -5.25930731e-01  7.95969482e-03 -3.29781760e-01\n",
      "  -1.43919573e-01  4.72556325e-01]]\n",
      "Logistic Regression: Confusion Matrix\n",
      "[[363622 180027]\n",
      " [326591 430728]]\n",
      "Logistic Regression: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.67      0.59    543649\n",
      "          1       0.71      0.57      0.63    757319\n",
      "\n",
      "avg / total       0.63      0.61      0.61   1300968\n",
      "\n",
      "Mon Dec  3 01:48:00 2018\n",
      "Logistic Regression: End\n"
     ]
    }
   ],
   "source": [
    "if enable_lr:\n",
    "    print(\"Logistic Regression: Start\")\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    lr = LogisticRegression(C=1, random_state=0, solver='saga', multi_class='ovr', \n",
    "                            verbose=verbose_level, n_jobs=10, max_iter=model_max_iter)\n",
    "    print(lr)\n",
    "    print(\"Logistic Regression: Fit\")\n",
    "    lr.fit(X_train, Y_train)\n",
    "    \n",
    "    # save model to file\n",
    "    pickle.dump(lr, open(file_lr, \"wb\"))\n",
    "    \n",
    "    \n",
    "if predict_lr:\n",
    "    # load model from file\n",
    "    loaded_model = pickle.load(open(file_lr, \"rb\"))\n",
    "    print(\"Logistic Regression: Predict\")\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(lr.score(X_train, Y_train)))\n",
    "    print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(lr.score(X_test, Y_test)))\n",
    "\n",
    "    # print the intercept (Note: one vs rest => 1 vs 2and3, 2 vs 1and3, 3 vs 1and2)\n",
    "    print(\"Logistic Regression: Intercept\")\n",
    "    print(lr.intercept_)\n",
    "\n",
    "    # print the coeficients (Note: one vs rest => 1 vs 2and3, 2 vs 1and3, 3 vs 1and2)\n",
    "    print(\"Logistic Regression: Coefficients\")\n",
    "    print(lr.coef_)\n",
    "\n",
    "    print(\"Logistic Regression: Confusion Matrix\")\n",
    "    cnf_matrix_lg = confusion_matrix(Y_test, y_pred)\n",
    "    print(cnf_matrix_lg)\n",
    "    \n",
    "    print(\"Logistic Regression: Classification Report\")\n",
    "    print(classification_report(Y_test, y_pred))\n",
    "\n",
    "    t_end =  time.time()\n",
    "    print(time.asctime( time.localtime(t_end) ))\n",
    "    \n",
    "print(\"Logistic Regression: End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with L1 Regularization: Start\n",
      "Mon Dec  3 01:49:13 2018\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=10,\n",
      "          penalty='l1', random_state=None, solver='saga', tol=0.0001,\n",
      "          verbose=3, warm_start=False)\n",
      "Logistic Regression with L1 Regularization: Fit\n",
      "convergence after 13 epochs took 36 seconds\n",
      "Logistic Regression with L1 Regularization: Predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   1 out of   1 | elapsed:   36.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on train set: 0.61\n",
      "Accuracy of logistic regression classifier on test set: 0.61\n",
      "Logistic Regression with L1 Regularization: Confusion Matrix\n",
      "[[363734 179915]\n",
      " [326747 430572]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.67      0.59    543649\n",
      "          1       0.71      0.57      0.63    757319\n",
      "\n",
      "avg / total       0.63      0.61      0.61   1300968\n",
      "\n",
      "Logistic Regression with L1 Regularization: Classification Report\n",
      "Mon Dec  3 01:49:58 2018\n",
      "Logistic Regression with L1 Regularization: End\n"
     ]
    }
   ],
   "source": [
    "if (enable_lr_l1):\n",
    "    # with L1 regularization\n",
    "    print(\"Logistic Regression with L1 Regularization: Start\")\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    lr = LogisticRegression(penalty='l1', C=1, solver='saga', multi_class='ovr', \n",
    "                            verbose=verbose_level, n_jobs = 10, max_iter=model_max_iter)\n",
    "    print(lr)\n",
    "    print(\"Logistic Regression with L1 Regularization: Fit\")\n",
    "    lr.fit(X_train, Y_train)\n",
    "    \n",
    "    # save model to file\n",
    "    pickle.dump(lr, open(file_lr_l1, \"wb\"))\n",
    "\n",
    "if (predict_lr_l1):\n",
    "    # load model from file\n",
    "    loaded_model = pickle.load(open(file_lr_l1, \"rb\"))\n",
    "    print(\"Logistic Regression with L1 Regularization: Predict\")\n",
    "    y_pred = lr.predict(X_test)\n",
    "    \n",
    "    print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(lr.score(X_train, Y_train)))\n",
    "    print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(lr.score(X_test, Y_test)))\n",
    "\n",
    "    print(\"Logistic Regression with L1 Regularization: Confusion Matrix\")\n",
    "    cnf_matrix_lg_l1 = confusion_matrix(Y_test, y_pred)\n",
    "    print(cnf_matrix_lg_l1)\n",
    "    \n",
    "    print(classification_report(Y_test,y_pred))\n",
    "    print(\"Logistic Regression with L1 Regularization: Classification Report\")\n",
    "\n",
    "    t_end =  time.time()\n",
    "    print(time.asctime( time.localtime(t_end) ))\n",
    "    \n",
    "print(\"Logistic Regression with L1 Regularization: End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes Classification\n",
    "if enable_nbayes:\n",
    "    print(\"Naive Bayes: Start\")\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    nbayes = GaussianNB()\n",
    "    print(nbayes)\n",
    "    print(\"Naive Bayes: Fit\")\n",
    "    nbayes.fit(X_train, Y_train)\n",
    "    # save model to file\n",
    "    pickle.dump(nbayes, open(file_nbayes, \"wb\"))\n",
    "\n",
    "if predict_nbayes:\n",
    "    # load model from file\n",
    "    loaded_model = pickle.load(open(file_nbayes, \"rb\"))\n",
    "    print(\"Naive Bayes: Predict\")\n",
    "    y_pred = nbayes.predict(X_test)\n",
    "    print('Accuracy of Naive Bayes classifier on train set: {:.2f}'.format(nbayes.score(X_train, Y_train)))\n",
    "    print('Accuracy of Naove Nayes classifier on test set: {:.2f}'.format(nbayes.score(X_test, Y_test)))\n",
    "    \n",
    "    cnf_matrix_dt = confusion_matrix(Y_test, y_pred)\n",
    "    print(\"Naive Bayes: Confusion Matrix\")\n",
    "    print(cnf_matrix_dt)\n",
    "    print(\"Naive Bayes: Classification Report\")\n",
    "    print(classification_report(Y_test,y_pred))\n",
    "    t_end =  time.time()\n",
    "    print(time.asctime( time.localtime(t_end) ))\n",
    "    print(\"Naive Bayes: End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: Start\n",
      "Mon Dec  3 02:02:37 2018\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=50,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Decision Tree: Fit\n",
      "Decision Tree: Predict\n",
      "Accuracy of Decision Tree classifier on train set: 0.99\n",
      "Accuracy of Decision Tree classifier on test set: 0.61\n",
      "Decision Tree: Confusion Matrix\n",
      "[[317926 225723]\n",
      " [275811 481508]]\n",
      "Decision Tree: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.58      0.56    543649\n",
      "          1       0.68      0.64      0.66    757319\n",
      "\n",
      "avg / total       0.62      0.61      0.62   1300968\n",
      "\n",
      "Mon Dec  3 02:03:54 2018\n",
      "Decision Tree: End\n"
     ]
    }
   ],
   "source": [
    "if enable_dt:\n",
    "    print(\"Decision Tree: Start\")\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    tree = DecisionTreeClassifier(criterion='entropy',max_depth=50)\n",
    "    print(tree)\n",
    "    print(\"Decision Tree: Fit\")\n",
    "    tree.fit(X_train, Y_train)\n",
    "    # save model to file\n",
    "    pickle.dump(tree, open(file_dt, \"wb\"))\n",
    "\n",
    "if predict_dt:\n",
    "    # load model from file\n",
    "    loaded_model = pickle.load(open(file_dt, \"rb\"))\n",
    "    print(\"Decision Tree: Predict\")\n",
    "    y_pred = tree.predict(X_test)\n",
    "    print('Accuracy of Decision Tree classifier on train set: {:.2f}'.format(tree.score(X_train, Y_train)))\n",
    "    print('Accuracy of Decision Tree classifier on test set: {:.2f}'.format(tree.score(X_test, Y_test)))\n",
    "    \n",
    "    cnf_matrix_dt = confusion_matrix(Y_test, y_pred)\n",
    "    print(\"Decision Tree: Confusion Matrix\")\n",
    "    print(cnf_matrix_dt)\n",
    "    print(\"Decision Tree: Classification Report\")\n",
    "    print(classification_report(Y_test,y_pred))\n",
    "    t_end =  time.time()\n",
    "    print(time.asctime( time.localtime(t_end) ))\n",
    "    print(\"Decision Tree: End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-N-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: Start\n",
      "Mon Dec  3 02:14:20 2018\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "KNN: Fit\n",
      "KNN: Predict\n",
      "Accuracy of KNN classifier on train set: 0.79\n",
      "Accuracy of KNN classifier on test set: 0.62\n",
      "KNN: Confusion Matrix\n",
      "[[352766 190883]\n",
      " [307132 450187]]\n",
      "KNN: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.65      0.59    543649\n",
      "          1       0.70      0.59      0.64    757319\n",
      "\n",
      "avg / total       0.63      0.62      0.62   1300968\n",
      "\n",
      "Mon Dec  3 04:19:36 2018\n",
      "KNN: End\n"
     ]
    }
   ],
   "source": [
    "if enable_knn:\n",
    "    print(\"KNN: Start\")\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski', n_jobs = -1)\n",
    "    print(knn)\n",
    "    print(\"KNN: Fit\")\n",
    "    knn.fit(X_train, Y_train)\n",
    "\n",
    "    # save model to file\n",
    "    pickle.dump(knn, open(file_knn, \"wb\"))\n",
    "\n",
    "if predict_knn:\n",
    "    # load model from file\n",
    "    loaded_model = pickle.load(open(file_knn, \"rb\"))\n",
    "    \n",
    "    print(\"KNN: Predict\")\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print('Accuracy of KNN classifier on train set: {:.2f}'.format(knn.score(X_train, Y_train)))\n",
    "    print('Accuracy of KNN classifier on test set: {:.2f}'.format(knn.score(X_test, Y_test)))\n",
    "    \n",
    "    print(\"KNN: Confusion Matrix\")\n",
    "    cnf_matrix_knn = confusion_matrix(Y_test, y_pred)\n",
    "    print(cnf_matrix_knn)\n",
    "\n",
    "    print(\"KNN: Classification Report\")\n",
    "    print(classification_report(Y_test,y_pred))\n",
    "\n",
    "    t_end =  time.time()\n",
    "    print(time.asctime( time.localtime(t_end) ))\n",
    "\n",
    "    print(\"KNN: End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN - Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer Preceptron: Start\n",
      "Mon Dec  3 01:49:58 2018\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(25, 25, 25), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=3, warm_start=False)\n",
      "Multilayer Preceptron: fit\n",
      "Iteration 1, loss = 0.60878332\n",
      "Iteration 2, loss = 0.59258186\n",
      "Iteration 3, loss = 0.58785748\n",
      "Iteration 4, loss = 0.58568672\n",
      "Iteration 5, loss = 0.58423964\n",
      "Iteration 6, loss = 0.58296005\n",
      "Iteration 7, loss = 0.58208057\n",
      "Iteration 8, loss = 0.58161601\n",
      "Iteration 9, loss = 0.58113326\n",
      "Iteration 10, loss = 0.58084426\n",
      "Iteration 11, loss = 0.58050320\n",
      "Iteration 12, loss = 0.58034509\n",
      "Iteration 13, loss = 0.58013934\n",
      "Iteration 14, loss = 0.58000531\n",
      "Iteration 15, loss = 0.57992091\n",
      "Iteration 16, loss = 0.57980169\n",
      "Iteration 17, loss = 0.57962540\n",
      "Iteration 18, loss = 0.57947725\n",
      "Iteration 19, loss = 0.57937708\n",
      "Iteration 20, loss = 0.57933269\n",
      "Iteration 21, loss = 0.57922158\n",
      "Iteration 22, loss = 0.57915228\n",
      "Iteration 23, loss = 0.57912113\n",
      "Iteration 24, loss = 0.57906914\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Multilayer Preceptron: Predict\n",
      "Accuracy of Multilayer Perceptron classifier on train set: 0.67\n",
      "Accuracy of Multilayer Perceptron classifier on test set: 0.67\n",
      "Multilayer Preceptron: Confusion Matrix\n",
      "[[398457 145192]\n",
      " [288981 468338]]\n",
      "Multilayer Preceptron: Classificiation Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.73      0.65    543649\n",
      "          1       0.76      0.62      0.68    757319\n",
      "\n",
      "avg / total       0.69      0.67      0.67   1300968\n",
      "\n",
      "Mon Dec  3 01:56:20 2018\n",
      "Multilayer Preceptron: End\n"
     ]
    }
   ],
   "source": [
    "if enable_mlp:\n",
    "    print(\"Multilayer Preceptron: Start\")\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    \n",
    "    #mlpc = MLPClassifier(alpha=1)\n",
    "    #mlpc = MLPClassifier(hidden_layer_sizes=(12, 12, 12), max_iter=model_max_iter, verbose=verbose_level)\n",
    "    mlpc = MLPClassifier(hidden_layer_sizes=(25, 25, 25), verbose=verbose_level, max_iter=model_max_iter)\n",
    "    print(mlpc)\n",
    "    #mlp = multilayer_perceptron(n_hidden =2, activation='logistic', algorithm='sgd', random_state=3)\n",
    "    print(\"Multilayer Preceptron: fit\")\n",
    "    mlpc.fit(X_train, Y_train)\n",
    "    \n",
    "    # save model to file\n",
    "    pickle.dump(mlpc, open(file_mlp, \"wb\"))\n",
    "    \n",
    "if predict_mlp:\n",
    "    \n",
    "    # load model from file\n",
    "    loaded_model = pickle.load(open(file_mlp, \"rb\"))\n",
    "    print(\"Multilayer Preceptron: Predict\")\n",
    "    y_pred = mlpc.predict(X_test)\n",
    "\n",
    "    print('Accuracy of Multilayer Perceptron classifier on train set: {:.2f}'.format(mlpc.score(X_train, Y_train)))\n",
    "    print('Accuracy of Multilayer Perceptron classifier on test set: {:.2f}'.format(mlpc.score(X_test, Y_test)))\n",
    "    \n",
    "    print(\"Multilayer Preceptron: Confusion Matrix\")\n",
    "    cnf_matrix_mlp = confusion_matrix(Y_test, y_pred)\n",
    "    print(cnf_matrix_mlp)\n",
    "    \n",
    "    print(\"Multilayer Preceptron: Classificiation Report\")\n",
    "    print(classification_report(Y_test,y_pred))\n",
    "    \n",
    "    t_end =  time.time()\n",
    "    print(time.asctime( time.localtime(t_end) ))\n",
    "    \n",
    "    print(\"Multilayer Preceptron: End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: Start\n",
      "Mon Dec  3 02:05:17 2018\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=1000, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=3)\n",
      "SVM: Fit\n",
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: Predict\n",
      "Accuracy of SVM classifier on train set: 0.50\n",
      "Accuracy of SVM classifier on test set: 0.42\n",
      "SVM: Confusion Matrix\n",
      "[[536803   6846]\n",
      " [749524   7795]]\n",
      "SVM: Classfication Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.99      0.59    543649\n",
      "          1       0.53      0.01      0.02    757319\n",
      "\n",
      "avg / total       0.48      0.42      0.26   1300968\n",
      "\n",
      "Mon Dec  3 02:14:20 2018\n",
      "SVM: End\n"
     ]
    }
   ],
   "source": [
    "if enable_svm:\n",
    "    print(\"SVM: Start\")\n",
    "    t_start =  time.time()\n",
    "    print(time.asctime( time.localtime(t_start) ))\n",
    "    #svm = SVC(C=1, random_state=0, kernel='sigmoid', verbose=True)\n",
    "    #svm = SVC(C=1, random_state=0, kernel='linear', verbose=True, cache_size=200)\n",
    "    #svm = SVC(C=svm_c, gamma=svm_gamma, verbose = verbose_level)\n",
    "    #SVN prediction is taking for ever, limiting the max_iter to 100 instead of -1 (no limit)\n",
    "    svm = SVC(C=1, gamma = 'auto', verbose = verbose_level, max_iter=model_max_iter)\n",
    "    print(svm)\n",
    "    print(\"SVM: Fit\")\n",
    "    svm.fit(X_train, Y_train)\n",
    "\n",
    "    # save model to file\n",
    "    pickle.dump(svm, open(file_svm, \"wb\"))\n",
    "    \n",
    "if predict_svm:\n",
    "    # load model from file\n",
    "    loaded_model = pickle.load(open(file_svm, \"rb\"))\n",
    "    print(\"SVM: Predict\")\n",
    "    y_pred = svm.predict(X_test)\n",
    "    \n",
    "    print('Accuracy of SVM classifier on train set: {:.2f}'.format(svm.score(X_train, Y_train)))\n",
    "    print('Accuracy of SVM classifier on test set: {:.2f}'.format(svm.score(X_test, Y_test)))\n",
    "    \n",
    "    print(\"SVM: Confusion Matrix\")\n",
    "    cnf_matrix_svm = confusion_matrix(Y_test, y_pred)\n",
    "    print(cnf_matrix_svm)\n",
    "    \n",
    "    print(\"SVM: Classfication Report\")\n",
    "    print(classification_report(Y_test,y_pred))\n",
    "    \n",
    "    t_end =  time.time()\n",
    "    print(time.asctime( time.localtime(t_end) ))\n",
    "    \n",
    "    print(\"SVM: End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_end =  time.time()\n",
    "print(time.asctime( time.localtime(t_end) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
